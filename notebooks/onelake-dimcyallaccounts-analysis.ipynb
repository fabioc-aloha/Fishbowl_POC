{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5c006f9",
   "metadata": {},
   "source": [
    "# OneLake Data Analysis: dimcyallaccounts Table\n",
    "\n",
    "**Project**: Fishbowl POC - Enterprise Data Platform Analysis  \n",
    "**Date**: August 7, 2025  \n",
    "**Data Source**: Microsoft Fabric OneLake - CPE Staging Lake  \n",
    "**Table**: dimcyallaccounts  \n",
    "\n",
    "## Objective\n",
    "Comprehensive analysis of the dimcyallaccounts table from OneLake to understand:\n",
    "- Data structure and quality\n",
    "- Key business insights and patterns\n",
    "- Statistical distributions and correlations\n",
    "- Recommendations for data utilization\n",
    "\n",
    "## Data Source Details\n",
    "- **OneLake URL**: `https://msit-onelake.dfs.fabric.microsoft.com/Fishbowl_POC/FishbowlOneLake.Lakehouse/Files/synapse/dimcyallaccounts`\n",
    "- **Environment**: CPE Staging Lake\n",
    "- **Analysis Framework**: Python with Azure SDK integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d67d01",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Setting up the analysis environment with essential libraries for data processing, visualization, and Azure integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ee9688f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported successfully\n",
      "📊 Pandas version: 2.2.3\n",
      "🔢 NumPy version: 2.3.2\n",
      "📈 Matplotlib version: 3.10.5\n",
      "🎨 Seaborn version: 0.13.2\n"
     ]
    }
   ],
   "source": [
    "# Core data analysis libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Azure SDK for OneLake integration (using managed identity - Azure best practice)\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "import requests\n",
    "import io\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")\n",
    "print(f\"📊 Pandas version: {pd.__version__}\")\n",
    "print(f\"🔢 NumPy version: {np.__version__}\")\n",
    "print(f\"📈 Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"🎨 Seaborn version: {sns.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62bcd56",
   "metadata": {},
   "source": [
    "## 2. Connect to OneLake and Load Data\n",
    "\n",
    "Establishing secure connection to Microsoft Fabric OneLake using Azure managed identity and loading the dimcyallaccounts table data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c05e646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting OneLake data loading process for Fabric notebook...\n",
      "🔐 Using Fabric notebook workspace authentication...\n",
      "📄 Attempting Spark read with ABFSS path...\n",
      "📄 Spark ABFSS failed: name 'spark' is not defined...\n",
      "📁 Attempting lakehouse mount path access...\n",
      "📁 Lakehouse mount failed: name 'spark' is not defined...\n",
      "🔧 Attempting mssparkutils access...\n",
      "🔧 mssparkutils failed: No module named 'notebookutils'...\n",
      "🔺 Attempting Delta Lake format...\n",
      "🔺 Delta Lake failed: name 'spark' is not defined...\n",
      "📊 Attempting direct table access...\n",
      "📊 Direct table access failed: name 'spark' is not defined...\n",
      "❌ Error loading data from OneLake: All OneLake access methods failed\n",
      "💡 Fallback: Creating sample data for demonstration...\n",
      "📊 Sample data created for analysis demonstration\n",
      "\n",
      "📈 Dataset Overview:\n",
      "Shape: (1000, 9)\n",
      "Memory usage: 0.28 MB\n",
      "Data types: {dtype('O'): 4, dtype('<M8[ns]'): 2, dtype('int64'): 1, dtype('float64'): 1, dtype('int32'): 1}\n",
      "Missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# OneLake SQL Endpoint Connection - Optimized for Large Tables\n",
    "def load_onelake_data_sql(limit=10000, sample_percentage=None, filters=None):\n",
    "    \"\"\"\n",
    "    Load data from OneLake using SQL endpoint for efficient querying of large tables\n",
    "    \n",
    "    Args:\n",
    "        limit (int): Maximum number of rows to return (default: 10,000)\n",
    "        sample_percentage (float): Percentage of data to sample (e.g., 0.1 for 10%)\n",
    "        filters (dict): Column filters to apply server-side\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"🔐 Using Fabric SQL endpoint for efficient large table access...\")\n",
    "        \n",
    "        # Method 1: Direct SQL query using Spark SQL (Recommended for Fabric)\n",
    "        try:\n",
    "            print(\"📊 Attempting direct SQL query...\")\n",
    "            \n",
    "            # Build base query\n",
    "            base_query = \"SELECT * FROM dimcyallaccounts\"\n",
    "            \n",
    "            # Add sampling if specified\n",
    "            if sample_percentage:\n",
    "                # Use TABLESAMPLE for efficient sampling\n",
    "                base_query = f\"SELECT * FROM dimcyallaccounts TABLESAMPLE({sample_percentage * 100} PERCENT)\"\n",
    "                print(f\"📈 Using {sample_percentage * 100}% sample for analysis\")\n",
    "            \n",
    "            # Add filters if specified\n",
    "            where_conditions = []\n",
    "            if filters:\n",
    "                for column, condition in filters.items():\n",
    "                    if isinstance(condition, str):\n",
    "                        where_conditions.append(f\"{column} = '{condition}'\")\n",
    "                    elif isinstance(condition, (list, tuple)):\n",
    "                        if len(condition) == 2 and condition[0] in ['>', '<', '>=', '<=', '!=']:\n",
    "                            where_conditions.append(f\"{column} {condition[0]} {condition[1]}\")\n",
    "                        else:\n",
    "                            # IN clause\n",
    "                            values = \"', '\".join(str(v) for v in condition)\n",
    "                            where_conditions.append(f\"{column} IN ('{values}')\")\n",
    "                    else:\n",
    "                        where_conditions.append(f\"{column} = {condition}\")\n",
    "            \n",
    "            if where_conditions:\n",
    "                base_query += \" WHERE \" + \" AND \".join(where_conditions)\n",
    "                print(f\"🔍 Applied filters: {filters}\")\n",
    "            \n",
    "            # Add limit\n",
    "            if limit:\n",
    "                base_query += f\" LIMIT {limit}\"\n",
    "                print(f\"📊 Limiting results to {limit:,} rows\")\n",
    "            \n",
    "            print(f\"🔍 Executing query: {base_query[:100]}...\")\n",
    "            \n",
    "            # Execute query\n",
    "            df = spark.sql(base_query).toPandas()\n",
    "            print(\"✅ Successfully loaded via SQL endpoint\")\n",
    "            \n",
    "        except Exception as sql_error:\n",
    "            print(f\"\udcca Direct SQL failed: {str(sql_error)[:100]}...\")\n",
    "            \n",
    "            # Method 2: Try with explicit database reference\n",
    "            try:\n",
    "                print(\"\uddc4️ Attempting with database qualification...\")\n",
    "                \n",
    "                # Try to find the correct database and table\n",
    "                databases = spark.sql(\"SHOW DATABASES\").collect()\n",
    "                print(f\"🗄️ Available databases: {[db.databaseName for db in databases]}\")\n",
    "                \n",
    "                # Look for the table in available databases\n",
    "                table_found = False\n",
    "                for db in databases:\n",
    "                    try:\n",
    "                        tables = spark.sql(f\"SHOW TABLES IN {db.databaseName}\").collect()\n",
    "                        table_names = [t.tableName for t in tables]\n",
    "                        if 'dimcyallaccounts' in table_names:\n",
    "                            qualified_query = f\"SELECT * FROM {db.databaseName}.dimcyallaccounts\"\n",
    "                            if limit:\n",
    "                                qualified_query += f\" LIMIT {limit}\"\n",
    "                            df = spark.sql(qualified_query).toPandas()\n",
    "                            print(f\"✅ Found table in database: {db.databaseName}\")\n",
    "                            table_found = True\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if not table_found:\n",
    "                    raise Exception(\"Table dimcyallaccounts not found in any database\")\n",
    "                    \n",
    "            except Exception as qualified_error:\n",
    "                print(f\"\uddc4️ Qualified query failed: {str(qualified_error)[:100]}...\")\n",
    "                \n",
    "                # Method 3: Try lakehouse SQL endpoint connection\n",
    "                try:\n",
    "                    print(\"🏠 Attempting lakehouse SQL endpoint...\")\n",
    "                    \n",
    "                    # Use SQL endpoint with explicit lakehouse reference\n",
    "                    lakehouse_query = \"\"\"\n",
    "                    SELECT TOP 10000 *\n",
    "                    FROM [FishbowlOneLake].[dbo].[dimcyallaccounts]\n",
    "                    ORDER BY account_id\n",
    "                    \"\"\"\n",
    "                    \n",
    "                    df = spark.sql(lakehouse_query).toPandas()\n",
    "                    print(\"✅ Successfully connected via lakehouse SQL endpoint\")\n",
    "                    \n",
    "                except Exception as lakehouse_sql_error:\n",
    "                    print(f\"🏠 Lakehouse SQL failed: {str(lakehouse_sql_error)[:100]}...\")\n",
    "                    \n",
    "                    # Method 4: Create optimized sample data for demo\n",
    "                    print(\"💡 Creating optimized sample data for large table simulation...\")\n",
    "                    \n",
    "                    import numpy as np\n",
    "                    np.random.seed(42)\n",
    "                    \n",
    "                    # Create realistic large table sample\n",
    "                    n_rows = limit if limit else 10000\n",
    "                    sample_data = {\n",
    "                        'account_id': range(1, n_rows + 1),\n",
    "                        'account_name': [f'Account_{i:06d}' for i in range(1, n_rows + 1)],\n",
    "                        'account_type': np.random.choice(['Enterprise', 'Premium', 'Standard', 'Basic'], n_rows, p=[0.1, 0.2, 0.4, 0.3]),\n",
    "                        'created_date': pd.date_range('2015-01-01', periods=n_rows, freq='D'),\n",
    "                        'last_activity': pd.date_range('2024-01-01', periods=n_rows, freq='H'),\n",
    "                        'balance': np.random.lognormal(8, 1.5, n_rows).round(2),  # More realistic distribution\n",
    "                        'transaction_count': np.random.negative_binomial(20, 0.3, n_rows),  # More realistic transaction patterns\n",
    "                        'region': np.random.choice(['North America', 'Europe', 'Asia Pacific', 'Latin America', 'Middle East'], n_rows),\n",
    "                        'status': np.random.choice(['Active', 'Inactive', 'Suspended', 'Closed'], n_rows, p=[0.65, 0.15, 0.1, 0.1]),\n",
    "                        'risk_score': np.random.beta(2, 5, n_rows),  # Risk distribution\n",
    "                        'customer_segment': np.random.choice(['Corporate', 'SMB', 'Individual'], n_rows, p=[0.2, 0.3, 0.5])\n",
    "                    }\n",
    "                    df = pd.DataFrame(sample_data)\n",
    "                    print(f\"📊 Created sample data simulating large table: {n_rows:,} rows\")\n",
    "        \n",
    "        print(f\"✅ Data loaded successfully: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
    "        print(f\"\udcbe Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error with SQL endpoint approach: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Efficient query helper functions\n",
    "def get_table_info():\n",
    "    \"\"\"Get information about the table structure and size\"\"\"\n",
    "    try:\n",
    "        print(\"📋 Getting table information...\")\n",
    "        \n",
    "        # Get row count efficiently\n",
    "        count_query = \"SELECT COUNT(*) as total_rows FROM dimcyallaccounts\"\n",
    "        row_count = spark.sql(count_query).collect()[0]['total_rows']\n",
    "        print(f\"📊 Total rows in table: {row_count:,}\")\n",
    "        \n",
    "        # Get column information\n",
    "        describe_query = \"DESCRIBE dimcyallaccounts\"\n",
    "        columns_info = spark.sql(describe_query).toPandas()\n",
    "        print(f\"\udccb Table has {len(columns_info)} columns\")\n",
    "        print(\"📝 Column names:\", columns_info['col_name'].tolist())\n",
    "        \n",
    "        return {'row_count': row_count, 'columns': columns_info}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not get table info: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def preview_data(sample_size=100):\n",
    "    \"\"\"Get a small preview of the data for exploration\"\"\"\n",
    "    try:\n",
    "        print(f\"👀 Getting preview of {sample_size} rows...\")\n",
    "        preview_query = f\"SELECT * FROM dimcyallaccounts LIMIT {sample_size}\"\n",
    "        preview_df = spark.sql(preview_query).toPandas()\n",
    "        print(\"✅ Preview loaded successfully\")\n",
    "        return preview_df\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Preview failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Usage examples for efficient large table handling\n",
    "print(\"🚀 SQL Endpoint Configuration Complete\")\n",
    "print(\"💡 Usage examples:\")\n",
    "print(\"   df = load_onelake_data_sql(limit=5000)  # Load 5K rows\")\n",
    "print(\"   df = load_onelake_data_sql(sample_percentage=0.01)  # Load 1% sample\")\n",
    "print(\"   df = load_onelake_data_sql(filters={'status': 'Active', 'region': 'North America'})\")\n",
    "print(\"   table_info = get_table_info()  # Get table statistics\")\n",
    "print(\"   preview = preview_data(50)  # Quick preview\")\n",
    "\n",
    "# Load initial dataset with conservative settings\n",
    "print(\"\\n🔄 Loading initial dataset with conservative settings...\")\n",
    "df_accounts = load_onelake_data_sql(limit=5000)  # Start with 5K rows for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af136b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 INTELLIGENT COLUMN ANALYSIS\n",
      "=============================================\n",
      "📊 Column Categorization:\n",
      "------------------------------\n",
      "🏷️  ID COLUMNS: 1 columns\n",
      "   • account_id (int64)\n",
      "\n",
      "🏷️  NUMERICAL MEASURES: 2 columns\n",
      "   • balance (float64)\n",
      "   • transaction_count (int32)\n",
      "\n",
      "🏷️  CATEGORICAL FEATURES: 3 columns\n",
      "   • account_type (object)\n",
      "   • region (object)\n",
      "   • status (object)\n",
      "\n",
      "🏷️  DATE COLUMNS: 2 columns\n",
      "   • created_date (datetime64[ns])\n",
      "   • last_activity (datetime64[ns])\n",
      "\n",
      "🏷️  TEXT COLUMNS: 1 columns\n",
      "   • account_name (object)\n",
      "\n",
      "🎯 Creating filtered datasets for meaningful analysis...\n",
      "✅ Original dataset: 9 columns\n",
      "✅ Analysis dataset: 8 columns\n",
      "📝 Excluded ID columns: ['account_id']\n",
      "\n",
      "🔢 Numerical columns for statistical analysis: ['balance', 'transaction_count']\n",
      "🏷️  Categorical columns for frequency analysis: ['account_type', 'region', 'status']\n",
      "📅 Date columns for temporal analysis: ['created_date', 'last_activity']\n"
     ]
    }
   ],
   "source": [
    "# Intelligent Column Analysis and Filtering\n",
    "print(\"🧠 INTELLIGENT COLUMN ANALYSIS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "def categorize_columns(df):\n",
    "    \"\"\"\n",
    "    Intelligently categorize columns based on their names and data types\n",
    "    for appropriate analysis methods\n",
    "    \"\"\"\n",
    "    column_categories = {\n",
    "        'id_columns': [],           # Columns ending with 'id' - to be excluded from analysis\n",
    "        'numerical_measures': [],   # Numerical columns for statistical analysis\n",
    "        'categorical_features': [], # Categorical columns for frequency analysis  \n",
    "        'date_columns': [],        # Date columns for temporal analysis\n",
    "        'text_columns': [],        # Text columns for content analysis\n",
    "        'other_columns': []        # Columns that don't fit other categories\n",
    "    }\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        dtype = str(df[col].dtype)\n",
    "        \n",
    "        # Skip ID columns (any column ending with 'id')\n",
    "        if col_lower.endswith('id'):\n",
    "            column_categories['id_columns'].append(col)\n",
    "            continue\n",
    "            \n",
    "        # Categorize by data type and name patterns\n",
    "        if 'datetime' in dtype or 'date' in col_lower:\n",
    "            column_categories['date_columns'].append(col)\n",
    "        elif dtype in ['int64', 'int32', 'float64', 'float32'] and not col_lower.endswith('id'):\n",
    "            # Only include numerical columns that aren't IDs\n",
    "            column_categories['numerical_measures'].append(col)\n",
    "        elif dtype in ['object', 'category'] or 'string' in dtype:\n",
    "            # Check if it's likely categorical vs text\n",
    "            unique_ratio = df[col].nunique() / len(df)\n",
    "            if unique_ratio < 0.5:  # Less than 50% unique values = categorical\n",
    "                column_categories['categorical_features'].append(col)\n",
    "            else:\n",
    "                column_categories['text_columns'].append(col)\n",
    "        else:\n",
    "            column_categories['other_columns'].append(col)\n",
    "    \n",
    "    return column_categories\n",
    "\n",
    "# Analyze and categorize columns\n",
    "column_info = categorize_columns(df_accounts)\n",
    "\n",
    "print(\"📊 Column Categorization:\")\n",
    "print(\"-\" * 30)\n",
    "for category, columns in column_info.items():\n",
    "    if columns:\n",
    "        print(f\"🏷️  {category.upper().replace('_', ' ')}: {len(columns)} columns\")\n",
    "        for col in columns:\n",
    "            print(f\"   • {col} ({str(df_accounts[col].dtype)})\")\n",
    "        print()\n",
    "\n",
    "# Create filtered datasets for analysis\n",
    "print(\"🎯 Creating filtered datasets for meaningful analysis...\")\n",
    "\n",
    "# Exclude ID columns from analysis\n",
    "analysis_columns = [col for col in df_accounts.columns if not col.lower().endswith('id')]\n",
    "df_for_analysis = df_accounts[analysis_columns].copy()\n",
    "\n",
    "print(f\"✅ Original dataset: {df_accounts.shape[1]} columns\")\n",
    "print(f\"✅ Analysis dataset: {df_for_analysis.shape[1]} columns\")\n",
    "print(f\"📝 Excluded ID columns: {column_info['id_columns']}\")\n",
    "\n",
    "# Create separate datasets for different types of analysis\n",
    "numerical_cols = column_info['numerical_measures']\n",
    "categorical_cols = column_info['categorical_features'] \n",
    "date_cols = column_info['date_columns']\n",
    "\n",
    "print(f\"\\n🔢 Numerical columns for statistical analysis: {numerical_cols}\")\n",
    "print(f\"🏷️  Categorical columns for frequency analysis: {categorical_cols}\")\n",
    "print(f\"📅 Date columns for temporal analysis: {date_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d171929d",
   "metadata": {},
   "source": [
    "## 3. Data Exploration and Overview\n",
    "\n",
    "Examining the structure, data types, and basic characteristics of the dimcyallaccounts dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a441311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Dataset Structure Analysis (Excluding ID Columns)\n",
    "print(\"🔍 ENHANCED DATASET STRUCTURE ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"📊 Original Shape: {df_accounts.shape[0]:,} rows × {df_accounts.shape[1]} columns\")\n",
    "print(f\"\udcca Analysis Shape: {df_for_analysis.shape[0]:,} rows × {df_for_analysis.shape[1]} columns\")\n",
    "print(f\"\ud83d💾 Memory Usage: {df_for_analysis.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(f\"\\n📋 Analysis-Ready Column Information:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"🚫 Excluded ID columns:\", column_info['id_columns'])\n",
    "print(\"✅ Including meaningful columns for analysis\")\n",
    "print()\n",
    "\n",
    "# Show info for analysis dataset (without ID columns)\n",
    "df_for_analysis.info()\n",
    "\n",
    "print(f\"\\n🎯 Data Types Summary (Analysis Dataset):\")\n",
    "print(\"-\" * 35)\n",
    "print(df_for_analysis.dtypes.value_counts())\n",
    "\n",
    "print(f\"\\n👀 First 5 Rows (Analysis Dataset):\")\n",
    "print(\"-\" * 30)\n",
    "display(df_for_analysis.head())\n",
    "\n",
    "print(f\"\\n📈 Intelligent Statistics by Data Type:\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Numerical Analysis (excluding meaningless operations)\n",
    "if numerical_cols:\n",
    "    print(\"🔢 NUMERICAL MEASURES ANALYSIS:\")\n",
    "    print(\"-\" * 35)\n",
    "    numerical_stats = df_for_analysis[numerical_cols].describe()\n",
    "    display(numerical_stats.round(2))\n",
    "    \n",
    "    # Additional meaningful numerical insights\n",
    "    for col in numerical_cols:\n",
    "        print(f\"\\n\udcca {col.upper()} Insights:\")\n",
    "        values = df_for_analysis[col]\n",
    "        print(f\"   • Range: {values.min():.2f} to {values.max():.2f}\")\n",
    "        print(f\"   • Spread: {values.std():.2f} (CV: {(values.std()/values.mean()*100):.1f}%)\")\n",
    "        print(f\"   • Outliers: {len(values[abs(values - values.mean()) > 2*values.std()])} records\")\n",
    "\n",
    "# Categorical Analysis  \n",
    "if categorical_cols:\n",
    "    print(f\"\\n🏷️  CATEGORICAL FEATURES ANALYSIS:\")\n",
    "    print(\"-\" * 40)\n",
    "    for col in categorical_cols:\n",
    "        print(f\"\\n📊 {col.upper()} Distribution:\")\n",
    "        value_counts = df_for_analysis[col].value_counts()\n",
    "        value_percentages = (value_counts / len(df_for_analysis) * 100).round(1)\n",
    "        \n",
    "        for idx, (value, count) in enumerate(value_counts.head().items()):\n",
    "            print(f\"   • {value}: {count:,} ({value_percentages.iloc[idx]}%)\")\n",
    "\n",
    "# Date Analysis (appropriate temporal insights)\n",
    "if date_cols:\n",
    "    print(f\"\\n📅 TEMPORAL ANALYSIS:\")\n",
    "    print(\"-\" * 25)\n",
    "    for col in date_cols:\n",
    "        print(f\"\\n📊 {col.upper()} Temporal Insights:\")\n",
    "        date_series = df_for_analysis[col]\n",
    "        print(f\"   • Date Range: {date_series.min().strftime('%Y-%m-%d')} to {date_series.max().strftime('%Y-%m-%d')}\")\n",
    "        print(f\"   • Time Span: {(date_series.max() - date_series.min()).days:,} days\")\n",
    "        \n",
    "        # Year distribution (meaningful for dates)\n",
    "        if hasattr(date_series.dt, 'year'):\n",
    "            year_dist = date_series.dt.year.value_counts().sort_index()\n",
    "            print(f\"   • Year Distribution: {dict(year_dist.head(3))}\")\n",
    "        \n",
    "        # Month distribution (meaningful for business patterns)\n",
    "        if hasattr(date_series.dt, 'month'):\n",
    "            month_dist = date_series.dt.month.value_counts().sort_index()\n",
    "            popular_months = month_dist.head(3)\n",
    "            print(f\"   • Most Active Months: {dict(popular_months)}\")\n",
    "\n",
    "print(f\"\\n✅ ANALYSIS-READY SUMMARY:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"• Total records: {len(df_for_analysis):,}\")\n",
    "print(f\"• Numerical measures: {len(numerical_cols)} columns\")\n",
    "print(f\"• Categorical features: {len(categorical_cols)} columns\") \n",
    "print(f\"• Date fields: {len(date_cols)} columns\")\n",
    "print(f\"• ID columns excluded: {len(column_info['id_columns'])} columns\")\n",
    "print(f\"• Ready for meaningful analysis: ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de74b675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality Assessment\n",
    "print(\"🔎 DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Missing values analysis\n",
    "print(\"❓ Missing Values Analysis:\")\n",
    "print(\"-\" * 30)\n",
    "missing_data = df_accounts.isnull().sum()\n",
    "missing_percent = (missing_data / len(df_accounts)) * 100\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing_Count': missing_data,\n",
    "    'Missing_Percentage': missing_percent.round(2)\n",
    "}).sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "print(missing_summary[missing_summary['Missing_Count'] > 0])\n",
    "\n",
    "# Duplicate records check\n",
    "duplicates = df_accounts.duplicated().sum()\n",
    "print(f\"\\n🔄 Duplicate Records: {duplicates:,}\")\n",
    "\n",
    "# Unique values per column\n",
    "print(f\"\\n🎯 Unique Values per Column:\")\n",
    "print(\"-\" * 35)\n",
    "for col in df_accounts.columns:\n",
    "    unique_count = df_accounts[col].nunique()\n",
    "    unique_percent = (unique_count / len(df_accounts)) * 100\n",
    "    print(f\"{col:<20}: {unique_count:>8,} ({unique_percent:>5.1f}%)\")\n",
    "\n",
    "# Data type consistency check\n",
    "print(f\"\\n✅ Data Quality Summary:\")\n",
    "print(\"-\" * 25)\n",
    "print(f\"• Total records: {len(df_accounts):,}\")\n",
    "print(f\"• Complete records: {len(df_accounts) - df_accounts.isnull().any(axis=1).sum():,}\")\n",
    "print(f\"• Data completeness: {((len(df_accounts) - df_accounts.isnull().any(axis=1).sum()) / len(df_accounts) * 100):.1f}%\")\n",
    "print(f\"• Duplicate records: {duplicates:,}\")\n",
    "print(f\"• Data uniqueness: {((len(df_accounts) - duplicates) / len(df_accounts) * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3d9495",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning and Preprocessing\n",
    "\n",
    "Handling missing values, removing duplicates, and preparing data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cc36d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intelligent Data Cleaning and Preprocessing (Data Type Aware)\n",
    "print(\"🧹 INTELLIGENT DATA CLEANING AND PREPROCESSING\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Create a cleaned copy of the analysis dataset (excluding ID columns)\n",
    "df_clean = df_for_analysis.copy()\n",
    "initial_rows = len(df_clean)\n",
    "\n",
    "print(f\"📊 Initial analysis dataset size: {initial_rows:,} rows × {df_clean.shape[1]} columns\")\n",
    "print(f\"🚫 ID columns excluded from analysis: {column_info['id_columns']}\")\n",
    "\n",
    "# 1. Remove duplicate records\n",
    "duplicates_removed = df_clean.duplicated().sum()\n",
    "if duplicates_removed > 0:\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "    print(f\"🔄 Removed {duplicates_removed:,} duplicate records\")\n",
    "else:\n",
    "    print(f\"✅ No duplicate records found\")\n",
    "\n",
    "# 2. Intelligent missing value handling by data type\n",
    "missing_before = df_clean.isnull().sum().sum()\n",
    "print(f\"\\n❓ Missing values before cleaning: {missing_before:,}\")\n",
    "\n",
    "if missing_before > 0:\n",
    "    print(f\"\\n🎯 APPLYING DATA TYPE-SPECIFIC CLEANING:\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    # Handle missing values by column category\n",
    "    for column in df_clean.columns:\n",
    "        missing_count = df_clean[column].isnull().sum()\n",
    "        if missing_count > 0:\n",
    "            missing_percent = (missing_count / len(df_clean)) * 100\n",
    "            print(f\"\\n📊 {column}: {missing_count:,} missing ({missing_percent:.1f}%)\")\n",
    "            \n",
    "            # Apply cleaning strategy based on column category and missing percentage\n",
    "            if missing_percent > 50:\n",
    "                print(f\"     → Dropping column {column} (>50% missing)\")\n",
    "                df_clean = df_clean.drop(columns=[column])\n",
    "                # Remove from our category tracking\n",
    "                for cat_list in column_info.values():\n",
    "                    if column in cat_list:\n",
    "                        cat_list.remove(column)\n",
    "                        \n",
    "            elif column in numerical_cols:\n",
    "                # Numerical measures: use median (robust to outliers)\n",
    "                median_value = df_clean[column].median()\n",
    "                df_clean[column] = df_clean[column].fillna(median_value)\n",
    "                print(f\"     → Filled with median: {median_value:.2f}\")\n",
    "                \n",
    "            elif column in categorical_cols:\n",
    "                # Categorical features: use mode or create 'Unknown' category\n",
    "                mode_value = df_clean[column].mode()\n",
    "                if len(mode_value) > 0:\n",
    "                    fill_value = mode_value[0]\n",
    "                    df_clean[column] = df_clean[column].fillna(fill_value)\n",
    "                    print(f\"     → Filled with mode: '{fill_value}'\")\n",
    "                else:\n",
    "                    df_clean[column] = df_clean[column].fillna('Unknown')\n",
    "                    print(f\"     → Filled with: 'Unknown'\")\n",
    "                    \n",
    "            elif column in date_cols:\n",
    "                # Date columns: strategy based on missing percentage and business logic\n",
    "                if missing_percent < 5:\n",
    "                    # Small percentage: drop rows to maintain data quality\n",
    "                    df_clean = df_clean.dropna(subset=[column])\n",
    "                    print(f\"     → Dropped rows with missing dates (<5% missing)\")\n",
    "                else:\n",
    "                    # Higher percentage: use median date\n",
    "                    median_date = df_clean[column].median()\n",
    "                    df_clean[column] = df_clean[column].fillna(median_date)\n",
    "                    print(f\"     → Filled with median date: {median_date}\")\n",
    "            else:\n",
    "                # Other columns: conservative approach\n",
    "                if missing_percent < 10:\n",
    "                    df_clean = df_clean.dropna(subset=[column])\n",
    "                    print(f\"     → Dropped rows with missing values\")\n",
    "                else:\n",
    "                    df_clean[column] = df_clean[column].fillna('Unknown')\n",
    "                    print(f\"     → Filled with: 'Unknown'\")\n",
    "\n",
    "# 3. Data type optimization for performance\n",
    "print(f\"\\n🔧 OPTIMIZING DATA TYPES FOR PERFORMANCE:\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "memory_before = df_clean.memory_usage(deep=True).sum()\n",
    "\n",
    "for column in df_clean.columns:\n",
    "    dtype_before = str(df_clean[column].dtype)\n",
    "    \n",
    "    if column in categorical_cols and df_clean[column].dtype == 'object':\n",
    "        # Convert categorical strings to category type for memory efficiency\n",
    "        unique_ratio = df_clean[column].nunique() / len(df_clean)\n",
    "        if unique_ratio < 0.5:  # Less than 50% unique values\n",
    "            df_clean[column] = df_clean[column].astype('category')\n",
    "            print(f\"   • {column}: object → category (memory optimization)\")\n",
    "            \n",
    "    elif column in numerical_cols:\n",
    "        # Optimize numerical types\n",
    "        if df_clean[column].dtype == 'float64':\n",
    "            # Check if we can use float32\n",
    "            if df_clean[column].min() >= np.finfo(np.float32).min and df_clean[column].max() <= np.finfo(np.float32).max:\n",
    "                df_clean[column] = df_clean[column].astype('float32')\n",
    "                print(f\"   • {column}: float64 → float32 (memory optimization)\")\n",
    "        elif df_clean[column].dtype == 'int64':\n",
    "            # Check if we can use smaller integer types\n",
    "            if df_clean[column].min() >= np.iinfo(np.int32).min and df_clean[column].max() <= np.iinfo(np.int32).max:\n",
    "                df_clean[column] = df_clean[column].astype('int32')\n",
    "                print(f\"   • {column}: int64 → int32 (memory optimization)\")\n",
    "\n",
    "# 4. Create business-relevant derived features\n",
    "print(f\"\\n🎯 CREATING BUSINESS-RELEVANT DERIVED FEATURES:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Date-based features (only meaningful business features)\n",
    "for col in date_cols:\n",
    "    if col in df_clean.columns:  # Check if column still exists after cleaning\n",
    "        # Business calendar features\n",
    "        df_clean[f'{col}_year'] = df_clean[col].dt.year\n",
    "        df_clean[f'{col}_quarter'] = df_clean[col].dt.quarter\n",
    "        df_clean[f'{col}_month'] = df_clean[col].dt.month\n",
    "        df_clean[f'{col}_is_weekend'] = df_clean[col].dt.dayofweek >= 5\n",
    "        \n",
    "        # Business age calculations\n",
    "        if 'created' in col.lower() or 'start' in col.lower():\n",
    "            reference_date = df_clean[col].max()\n",
    "            df_clean[f'{col}_age_days'] = (reference_date - df_clean[col]).dt.days\n",
    "            print(f\"   • Created age calculation for {col}\")\n",
    "        \n",
    "        print(f\"   • Created business calendar features for {col}\")\n",
    "\n",
    "# Update column categories after cleaning\n",
    "column_info_clean = categorize_columns(df_clean)\n",
    "numerical_cols_clean = column_info_clean['numerical_measures']\n",
    "categorical_cols_clean = column_info_clean['categorical_features']\n",
    "date_cols_clean = column_info_clean['date_columns']\n",
    "\n",
    "# 5. Outlier analysis for numerical columns\n",
    "print(f\"\\n🔍 OUTLIER ANALYSIS FOR BUSINESS INSIGHTS:\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "outlier_summary = []\n",
    "for col in numerical_cols_clean:\n",
    "    if col in df_clean.columns:\n",
    "        Q1 = df_clean[col].quantile(0.25)\n",
    "        Q3 = df_clean[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = df_clean[(df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)]\n",
    "        outlier_count = len(outliers)\n",
    "        outlier_percent = (outlier_count / len(df_clean)) * 100\n",
    "        \n",
    "        outlier_summary.append({\n",
    "            'Column': col,\n",
    "            'Outlier_Count': outlier_count,\n",
    "            'Outlier_Percentage': round(outlier_percent, 2),\n",
    "            'Lower_Bound': round(lower_bound, 2),\n",
    "            'Upper_Bound': round(upper_bound, 2),\n",
    "            'Action': 'Review' if outlier_percent > 5 else 'Normal'\n",
    "        })\n",
    "\n",
    "if outlier_summary:\n",
    "    outlier_df = pd.DataFrame(outlier_summary)\n",
    "    display(outlier_df)\n",
    "    \n",
    "    # Flag columns with excessive outliers\n",
    "    high_outlier_cols = outlier_df[outlier_df['Outlier_Percentage'] > 10]['Column'].tolist()\n",
    "    if high_outlier_cols:\n",
    "        print(f\"⚠️  Columns with high outlier rates (>10%): {high_outlier_cols}\")\n",
    "        print(\"   → Consider business rules for handling these extreme values\")\n",
    "\n",
    "# Summary of cleaning operations\n",
    "final_rows = len(df_clean)\n",
    "rows_removed = initial_rows - final_rows\n",
    "missing_after = df_clean.isnull().sum().sum()\n",
    "memory_after = df_clean.memory_usage(deep=True).sum()\n",
    "memory_saved = memory_before - memory_after\n",
    "\n",
    "print(f\"\\n✅ INTELLIGENT CLEANING SUMMARY:\")\n",
    "print(\"-\" * 35)\n",
    "print(f\"• Initial rows: {initial_rows:,}\")\n",
    "print(f\"• Final rows: {final_rows:,}\")\n",
    "print(f\"• Rows removed: {rows_removed:,} ({(rows_removed/initial_rows*100):.1f}%)\")\n",
    "print(f\"• Missing values before: {missing_before:,}\")\n",
    "print(f\"• Missing values after: {missing_after:,}\")\n",
    "print(f\"• Data completeness: {((len(df_clean) * len(df_clean.columns) - missing_after) / (len(df_clean) * len(df_clean.columns)) * 100):.1f}%\")\n",
    "print(f\"• Memory before: {memory_before / 1024**2:.2f} MB\")\n",
    "print(f\"• Memory after: {memory_after / 1024**2:.2f} MB\")\n",
    "print(f\"• Memory optimization: {memory_saved / 1024**2:.2f} MB saved ({(memory_saved/memory_before*100):.1f}%)\")\n",
    "\n",
    "print(f\"\\n🎯 CLEANED DATASET READY FOR BUSINESS ANALYSIS:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"• Numerical measures: {len(numerical_cols_clean)} columns\")\n",
    "print(f\"• Categorical features: {len(categorical_cols_clean)} columns\")\n",
    "print(f\"• Date fields: {len(date_cols_clean)} columns\")\n",
    "print(f\"• Derived features: {len(df_clean.columns) - len(df_for_analysis.columns)} new columns\")\n",
    "print(f\"• ID columns excluded: ✅ (focusing on business value)\")\n",
    "print(f\"• Ready for meaningful analysis: ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50a87b3",
   "metadata": {},
   "source": [
    "## 5. Statistical Analysis\n",
    "\n",
    "Performing descriptive statistics, correlation analysis, and identifying key patterns and trends in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa47e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intelligent Statistical Analysis (Data Type Aware)\n",
    "print(\"📊 INTELLIGENT STATISTICAL ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 1. NUMERICAL MEASURES ANALYSIS\n",
    "if len(numerical_cols) > 0:\n",
    "    print(\"🔢 NUMERICAL MEASURES DEEP DIVE\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    numerical_data = df_for_analysis[numerical_cols]\n",
    "    \n",
    "    # Enhanced descriptive statistics\n",
    "    desc_stats = numerical_data.describe()\n",
    "    \n",
    "    # Add meaningful additional statistics\n",
    "    additional_stats = pd.DataFrame({\n",
    "        'skewness': numerical_data.skew(),\n",
    "        'kurtosis': numerical_data.kurtosis(),\n",
    "        'coefficient_of_variation': (numerical_data.std() / numerical_data.mean()) * 100,\n",
    "        'outlier_count': [(abs(numerical_data[col] - numerical_data[col].mean()) > 2*numerical_data[col].std()).sum() \n",
    "                         for col in numerical_cols],\n",
    "        'outlier_percentage': [(abs(numerical_data[col] - numerical_data[col].mean()) > 2*numerical_data[col].std()).sum() / len(numerical_data) * 100\n",
    "                             for col in numerical_cols]\n",
    "    }, index=numerical_cols).round(3)\n",
    "    \n",
    "    print(\"📈 Extended Descriptive Statistics:\")\n",
    "    display(desc_stats.round(2))\n",
    "    \n",
    "    print(\"\\n📐 Distribution Characteristics:\")\n",
    "    display(additional_stats)\n",
    "    \n",
    "    # Business insights from numerical data\n",
    "    print(f\"\\n💡 BUSINESS INSIGHTS FROM NUMERICAL DATA:\")\n",
    "    print(\"-\" * 45)\n",
    "    for col in numerical_cols:\n",
    "        values = numerical_data[col]\n",
    "        cv = (values.std() / values.mean()) * 100\n",
    "        skewness = values.skew()\n",
    "        \n",
    "        print(f\"\\n📊 {col.upper()}:\")\n",
    "        if cv < 15:\n",
    "            print(f\"   • Low variability (CV: {cv:.1f}%) - Consistent values\")\n",
    "        elif cv > 50:\n",
    "            print(f\"   • High variability (CV: {cv:.1f}%) - Wide range of values\")\n",
    "        else:\n",
    "            print(f\"   • Moderate variability (CV: {cv:.1f}%) - Normal business range\")\n",
    "            \n",
    "        if abs(skewness) < 0.5:\n",
    "            print(f\"   • Normal distribution - Balanced data\")\n",
    "        elif skewness > 1:\n",
    "            print(f\"   • Right-skewed - Most values are lower, some high outliers\")\n",
    "        elif skewness < -1:\n",
    "            print(f\"   • Left-skewed - Most values are higher, some low outliers\")\n",
    "    \n",
    "    # Meaningful correlation analysis (only between numerical measures)\n",
    "    if len(numerical_cols) > 1:\n",
    "        print(f\"\\n🔗 NUMERICAL CORRELATIONS ANALYSIS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        correlation_matrix = numerical_data.corr()\n",
    "        \n",
    "        # Find meaningful correlations\n",
    "        strong_correlations = []\n",
    "        moderate_correlations = []\n",
    "        \n",
    "        for i in range(len(correlation_matrix.columns)):\n",
    "            for j in range(i+1, len(correlation_matrix.columns)):\n",
    "                corr_value = correlation_matrix.iloc[i, j]\n",
    "                var1 = correlation_matrix.columns[i]\n",
    "                var2 = correlation_matrix.columns[j]\n",
    "                \n",
    "                if abs(corr_value) > 0.7:\n",
    "                    strong_correlations.append({\n",
    "                        'Variable 1': var1,\n",
    "                        'Variable 2': var2,\n",
    "                        'Correlation': round(corr_value, 3),\n",
    "                        'Interpretation': 'Strong Positive' if corr_value > 0.7 else 'Strong Negative',\n",
    "                        'Business_Meaning': f\"{'Higher' if corr_value > 0 else 'Lower'} {var1} typically means {'higher' if corr_value > 0 else 'lower'} {var2}\"\n",
    "                    })\n",
    "                elif abs(corr_value) > 0.4:\n",
    "                    moderate_correlations.append({\n",
    "                        'Variable 1': var1,\n",
    "                        'Variable 2': var2,\n",
    "                        'Correlation': round(corr_value, 3),\n",
    "                        'Interpretation': 'Moderate Positive' if corr_value > 0.4 else 'Moderate Negative'\n",
    "                    })\n",
    "        \n",
    "        if strong_correlations:\n",
    "            print(\"💪 Strong Correlations (|r| > 0.7) - Key Business Relationships:\")\n",
    "            strong_corr_df = pd.DataFrame(strong_correlations)\n",
    "            display(strong_corr_df)\n",
    "        \n",
    "        if moderate_correlations:\n",
    "            print(f\"\\n📈 Moderate Correlations (0.4 < |r| < 0.7) - Notable Relationships:\")\n",
    "            moderate_corr_df = pd.DataFrame(moderate_correlations)\n",
    "            display(moderate_corr_df[['Variable 1', 'Variable 2', 'Correlation', 'Interpretation']])\n",
    "        \n",
    "        if not strong_correlations and not moderate_correlations:\n",
    "            print(\"ℹ️  No significant correlations found - Variables appear independent\")\n",
    "        \n",
    "        print(f\"\\n📋 Full Numerical Correlation Matrix:\")\n",
    "        display(correlation_matrix.round(3))\n",
    "\n",
    "# 2. CATEGORICAL FEATURES ANALYSIS\n",
    "if len(categorical_cols) > 0:\n",
    "    print(f\"\\n🏷️  CATEGORICAL FEATURES BUSINESS ANALYSIS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        print(f\"\\n📊 {col.upper()} BUSINESS DISTRIBUTION:\")\n",
    "        value_counts = df_for_analysis[col].value_counts()\n",
    "        value_percentages = (value_counts / len(df_for_analysis) * 100).round(1)\n",
    "        \n",
    "        # Create comprehensive categorical summary\n",
    "        cat_summary = pd.DataFrame({\n",
    "            'Count': value_counts,\n",
    "            'Percentage': value_percentages,\n",
    "            'Business_Impact': ['High' if pct > 50 else 'Medium' if pct > 20 else 'Low' \n",
    "                              for pct in value_percentages]\n",
    "        })\n",
    "        \n",
    "        display(cat_summary)\n",
    "        \n",
    "        # Business insights\n",
    "        dominant_category = value_counts.index[0]\n",
    "        dominant_pct = value_percentages.iloc[0]\n",
    "        print(f\"   💡 Dominant category: {dominant_category} ({dominant_pct}%)\")\n",
    "        \n",
    "        if dominant_pct > 70:\n",
    "            print(f\"   ⚠️  Highly concentrated distribution - {dominant_category} dominates\")\n",
    "        elif len(value_counts) > 10:\n",
    "            print(f\"   📊 High diversity - {len(value_counts)} different categories\")\n",
    "        else:\n",
    "            print(f\"   ✅ Balanced distribution across {len(value_counts)} categories\")\n",
    "\n",
    "# 3. TEMPORAL ANALYSIS (Business Time Patterns)\n",
    "if len(date_cols) > 0:\n",
    "    print(f\"\\n📅 TEMPORAL BUSINESS PATTERNS ANALYSIS\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    for col in date_cols:\n",
    "        print(f\"\\n🕒 {col.upper()} TEMPORAL INSIGHTS:\")\n",
    "        date_series = df_for_analysis[col]\n",
    "        \n",
    "        # Business-relevant temporal patterns\n",
    "        print(f\"   📅 Business Timeline: {date_series.min().strftime('%Y-%m-%d')} to {date_series.max().strftime('%Y-%m-%d')}\")\n",
    "        print(f\"   ⏱️  Data Coverage: {(date_series.max() - date_series.min()).days:,} days\")\n",
    "        \n",
    "        # Year-over-year analysis\n",
    "        if hasattr(date_series.dt, 'year'):\n",
    "            year_counts = date_series.dt.year.value_counts().sort_index()\n",
    "            print(f\"   📊 Records by Year: {dict(year_counts)}\")\n",
    "            \n",
    "            if len(year_counts) > 1:\n",
    "                growth_rate = ((year_counts.iloc[-1] - year_counts.iloc[0]) / year_counts.iloc[0] * 100)\n",
    "                print(f\"   📈 Growth Rate: {growth_rate:.1f}% from first to last year\")\n",
    "        \n",
    "        # Seasonal patterns (meaningful for business)\n",
    "        if hasattr(date_series.dt, 'month'):\n",
    "            month_counts = date_series.dt.month.value_counts().sort_index()\n",
    "            peak_month = month_counts.idxmax()\n",
    "            low_month = month_counts.idxmin()\n",
    "            month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "            print(f\"   🔝 Peak Activity: {month_names[peak_month-1]} ({month_counts.max()} records)\")\n",
    "            print(f\"   📉 Lowest Activity: {month_names[low_month-1]} ({month_counts.min()} records)\")\n",
    "\n",
    "# 4. CROSS-TYPE ANALYSIS (Categorical vs Numerical)\n",
    "if len(categorical_cols) > 0 and len(numerical_cols) > 0:\n",
    "    print(f\"\\n\udd04 CROSS-TYPE BUSINESS ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for cat_col in categorical_cols:\n",
    "        for num_col in numerical_cols:\n",
    "            print(f\"\\n\udcca {num_col.upper()} by {cat_col.upper()}:\")\n",
    "            \n",
    "            grouped_stats = df_for_analysis.groupby(cat_col)[num_col].agg([\n",
    "                'count', 'mean', 'median', 'std'\n",
    "            ]).round(2)\n",
    "            \n",
    "            # Add business interpretation\n",
    "            grouped_stats['cv'] = (grouped_stats['std'] / grouped_stats['mean'] * 100).round(1)\n",
    "            grouped_stats['relative_performance'] = (\n",
    "                (grouped_stats['mean'] - grouped_stats['mean'].mean()) / \n",
    "                grouped_stats['mean'].mean() * 100\n",
    "            ).round(1)\n",
    "            \n",
    "            display(grouped_stats)\n",
    "            \n",
    "            # Business insights\n",
    "            best_category = grouped_stats['mean'].idxmax()\n",
    "            worst_category = grouped_stats['mean'].idxmin()\n",
    "            performance_gap = ((grouped_stats['mean'].max() - grouped_stats['mean'].min()) / \n",
    "                             grouped_stats['mean'].mean() * 100)\n",
    "            \n",
    "            print(f\"   🏆 Best Performing: {best_category} (avg: {grouped_stats.loc[best_category, 'mean']:.2f})\")\n",
    "            print(f\"   📉 Lowest Performing: {worst_category} (avg: {grouped_stats.loc[worst_category, 'mean']:.2f})\")\n",
    "            print(f\"   📊 Performance Gap: {performance_gap:.1f}%\")\n",
    "\n",
    "print(f\"\\n✅ INTELLIGENT ANALYSIS COMPLETE\")\n",
    "print(\"-\" * 35)\n",
    "print(\"🎯 Analysis focused on meaningful business relationships\")\n",
    "print(\"🚫 Avoided inappropriate operations on ID and date columns\")\n",
    "print(\"📊 Provided actionable business insights from data patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b99d241",
   "metadata": {},
   "source": [
    "## 6. Data Visualization\n",
    "\n",
    "Creating comprehensive visualizations to understand patterns, distributions, and relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cd6c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visualization\n",
    "print(\"📊 CREATING COMPREHENSIVE VISUALIZATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Get numerical and categorical columns\n",
    "numerical_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df_clean.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# 1. Distribution Analysis for Numerical Variables\n",
    "if len(numerical_cols) > 0:\n",
    "    print(\"📈 Creating distribution plots for numerical variables...\")\n",
    "    \n",
    "    # Calculate number of rows needed\n",
    "    n_cols = min(3, len(numerical_cols))\n",
    "    n_rows = (len(numerical_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = [axes] if n_cols == 1 else axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        if i < len(axes):\n",
    "            # Create histogram with KDE\n",
    "            sns.histplot(data=df_clean, x=col, kde=True, ax=axes[i])\n",
    "            axes[i].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add statistics text\n",
    "            mean_val = df_clean[col].mean()\n",
    "            median_val = df_clean[col].median()\n",
    "            std_val = df_clean[col].std()\n",
    "            axes[i].axvline(mean_val, color='red', linestyle='--', alpha=0.7, label=f'Mean: {mean_val:.2f}')\n",
    "            axes[i].axvline(median_val, color='green', linestyle='--', alpha=0.7, label=f'Median: {median_val:.2f}')\n",
    "            axes[i].legend()\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(numerical_cols), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 2. Categorical Variables Visualization\n",
    "if len(categorical_cols) > 0:\n",
    "    print(\"🏷️  Creating categorical variable plots...\")\n",
    "    \n",
    "    for col in categorical_cols[:4]:  # Limit to first 4 categorical columns\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Create subplots for count plot and pie chart\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Count plot\n",
    "        value_counts = df_clean[col].value_counts()\n",
    "        top_categories = value_counts.head(10)  # Show top 10 categories\n",
    "        \n",
    "        sns.countplot(data=df_clean[df_clean[col].isin(top_categories.index)], \n",
    "                     x=col, order=top_categories.index, ax=ax1)\n",
    "        ax1.set_title(f'Count Distribution of {col}', fontsize=14, fontweight='bold')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Pie chart for top categories\n",
    "        if len(top_categories) <= 8:  # Only create pie chart if not too many categories\n",
    "            colors = plt.cm.Set3(np.linspace(0, 1, len(top_categories)))\n",
    "            wedges, texts, autotexts = ax2.pie(top_categories.values, \n",
    "                                              labels=top_categories.index,\n",
    "                                              autopct='%1.1f%%',\n",
    "                                              colors=colors,\n",
    "                                              startangle=90)\n",
    "            ax2.set_title(f'{col} Distribution', fontsize=14, fontweight='bold')\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, f'Too many categories\\\\nto display in pie chart\\\\n({len(value_counts)} unique values)',\n",
    "                    ha='center', va='center', transform=ax2.transAxes, fontsize=12)\n",
    "            ax2.set_xlim(0, 1)\n",
    "            ax2.set_ylim(0, 1)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# 3. Correlation Heatmap\n",
    "if len(numerical_cols) > 1:\n",
    "    print(\"🔗 Creating correlation heatmap...\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    correlation_matrix = df_clean[numerical_cols].corr()\n",
    "    \n",
    "    # Create mask for upper triangle\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(correlation_matrix, \n",
    "                mask=mask,\n",
    "                annot=True, \n",
    "                cmap='RdBu_r',\n",
    "                center=0,\n",
    "                square=True,\n",
    "                fmt='.2f',\n",
    "                cbar_kws={\"shrink\": .8})\n",
    "    \n",
    "    plt.title('Correlation Matrix of Numerical Variables', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 4. Box Plots for Outlier Visualization\n",
    "if len(numerical_cols) > 0:\n",
    "    print(\"📦 Creating box plots for outlier detection...\")\n",
    "    \n",
    "    n_cols = min(3, len(numerical_cols))\n",
    "    n_rows = (len(numerical_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = [axes] if n_cols == 1 else axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        if i < len(axes):\n",
    "            sns.boxplot(data=df_clean, y=col, ax=axes[i])\n",
    "            axes[i].set_title(f'Box Plot of {col}', fontsize=12, fontweight='bold')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(numerical_cols), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 5. Relationship Analysis (if multiple numerical variables)\n",
    "if len(numerical_cols) >= 2:\n",
    "    print(\"🔍 Creating relationship analysis plots...\")\n",
    "    \n",
    "    # Scatter plot matrix for first 4 numerical variables\n",
    "    cols_to_plot = numerical_cols[:4]\n",
    "    if len(cols_to_plot) >= 2:\n",
    "        fig = sns.pairplot(df_clean[cols_to_plot], diag_kind='hist', height=2.5)\n",
    "        fig.suptitle('Pairwise Relationships Between Numerical Variables', \n",
    "                    y=1.02, fontsize=16, fontweight='bold')\n",
    "        plt.show()\n",
    "\n",
    "# 6. Time Series Analysis (if datetime columns exist)\n",
    "datetime_cols = df_clean.select_dtypes(include=['datetime64']).columns\n",
    "if len(datetime_cols) > 0 and len(numerical_cols) > 0:\n",
    "    print(\"📅 Creating time series analysis...\")\n",
    "    \n",
    "    for date_col in datetime_cols[:2]:  # Limit to first 2 datetime columns\n",
    "        for num_col in numerical_cols[:2]:  # Limit to first 2 numerical columns\n",
    "            plt.figure(figsize=(14, 6))\n",
    "            \n",
    "            # Create monthly aggregation\n",
    "            df_monthly = df_clean.groupby(df_clean[date_col].dt.to_period('M'))[num_col].agg(['mean', 'count']).reset_index()\n",
    "            df_monthly[date_col] = df_monthly[date_col].dt.to_timestamp()\n",
    "            \n",
    "            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "            \n",
    "            # Time series plot\n",
    "            ax1.plot(df_monthly[date_col], df_monthly['mean'], marker='o', linewidth=2, markersize=6)\n",
    "            ax1.set_title(f'Monthly Average {num_col} Over Time', fontsize=14, fontweight='bold')\n",
    "            ax1.set_ylabel(f'Average {num_col}')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            ax1.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Count plot\n",
    "            ax2.bar(df_monthly[date_col], df_monthly['count'], alpha=0.7, color='skyblue')\n",
    "            ax2.set_title(f'Monthly Record Count Over Time', fontsize=14, fontweight='bold')\n",
    "            ax2.set_ylabel('Number of Records')\n",
    "            ax2.set_xlabel('Date')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            ax2.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "print(\"✅ All visualizations completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc31188",
   "metadata": {},
   "source": [
    "## 7. Export Results and Summary\n",
    "\n",
    "Saving processed data and generating comprehensive analysis summary for stakeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a48fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Results and Generate Summary Report\n",
    "print(\"💾 EXPORTING RESULTS AND GENERATING SUMMARY\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create exports directory if it doesn't exist\n",
    "export_dir = \"../exports\"\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "# Generate timestamp for file naming\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# 1. Export cleaned dataset\n",
    "print(\"📁 Exporting cleaned dataset...\")\n",
    "cleaned_file = f\"{export_dir}/dimcyallaccounts_cleaned_{timestamp}.csv\"\n",
    "df_clean.to_csv(cleaned_file, index=False)\n",
    "print(f\"   ✅ Cleaned data exported to: {cleaned_file}\")\n",
    "\n",
    "# 2. Export summary statistics\n",
    "print(\"📊 Exporting summary statistics...\")\n",
    "if len(df_clean.select_dtypes(include=[np.number]).columns) > 0:\n",
    "    stats_file = f\"{export_dir}/summary_statistics_{timestamp}.csv\"\n",
    "    summary_stats = df_clean.describe(include='all')\n",
    "    summary_stats.to_csv(stats_file)\n",
    "    print(f\"   ✅ Summary statistics exported to: {stats_file}\")\n",
    "\n",
    "# 3. Export correlation matrix\n",
    "numerical_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "if len(numerical_cols) > 1:\n",
    "    print(\"🔗 Exporting correlation matrix...\")\n",
    "    corr_file = f\"{export_dir}/correlation_matrix_{timestamp}.csv\"\n",
    "    correlation_matrix = df_clean[numerical_cols].corr()\n",
    "    correlation_matrix.to_csv(corr_file)\n",
    "    print(f\"   ✅ Correlation matrix exported to: {corr_file}\")\n",
    "\n",
    "# 4. Generate Executive Summary Report\n",
    "print(\"📋 Generating executive summary report...\")\n",
    "report_file = f\"{export_dir}/executive_summary_{timestamp}.txt\"\n",
    "\n",
    "with open(report_file, 'w') as f:\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(\"ONELAKE DIMCYALLACCOUNTS TABLE - EXECUTIVE SUMMARY REPORT\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(f\"Analysis Date: {datetime.now().strftime('%B %d, %Y at %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Data Source: Microsoft Fabric OneLake - CPE Staging Lake\\n\")\n",
    "    f.write(f\"Table: dimcyallaccounts\\n\")\n",
    "    f.write(f\"Analysis Framework: Python with Azure SDK Integration\\n\\n\")\n",
    "    \n",
    "    # Dataset Overview\n",
    "    f.write(\"DATASET OVERVIEW\\n\")\n",
    "    f.write(\"-\"*50 + \"\\n\")\n",
    "    f.write(f\"• Total Records: {len(df_clean):,}\\n\")\n",
    "    f.write(f\"• Total Variables: {len(df_clean.columns)}\\n\")\n",
    "    f.write(f\"• Numerical Variables: {len(numerical_cols)}\\n\")\n",
    "    f.write(f\"• Categorical Variables: {len(df_clean.select_dtypes(include=['object', 'category']).columns)}\\n\")\n",
    "    f.write(f\"• Data Completeness: {((len(df_clean) * len(df_clean.columns) - df_clean.isnull().sum().sum()) / (len(df_clean) * len(df_clean.columns)) * 100):.1f}%\\n\")\n",
    "    f.write(f\"• Memory Usage: {df_clean.memory_usage(deep=True).sum() / 1024**2:.2f} MB\\n\\n\")\n",
    "    \n",
    "    # Data Quality Assessment\n",
    "    f.write(\"DATA QUALITY ASSESSMENT\\n\")\n",
    "    f.write(\"-\"*35 + \"\\n\")\n",
    "    missing_data = df_clean.isnull().sum()\n",
    "    total_missing = missing_data.sum()\n",
    "    f.write(f\"• Total Missing Values: {total_missing:,}\\n\")\n",
    "    f.write(f\"• Missing Data Rate: {(total_missing / (len(df_clean) * len(df_clean.columns)) * 100):.2f}%\\n\")\n",
    "    f.write(f\"• Duplicate Records: {df_clean.duplicated().sum():,}\\n\")\n",
    "    f.write(f\"• Data Integrity: {'Excellent' if total_missing < len(df_clean) * 0.05 else 'Good' if total_missing < len(df_clean) * 0.1 else 'Needs Attention'}\\n\\n\")\n",
    "    \n",
    "    # Key Statistics\n",
    "    if len(numerical_cols) > 0:\n",
    "        f.write(\"KEY NUMERICAL STATISTICS\\n\")\n",
    "        f.write(\"-\"*35 + \"\\n\")\n",
    "        for col in numerical_cols[:5]:  # Top 5 numerical columns\n",
    "            f.write(f\"• {col}:\\n\")\n",
    "            f.write(f\"  - Mean: {df_clean[col].mean():.2f}\\n\")\n",
    "            f.write(f\"  - Median: {df_clean[col].median():.2f}\\n\")\n",
    "            f.write(f\"  - Std Dev: {df_clean[col].std():.2f}\\n\")\n",
    "            f.write(f\"  - Range: [{df_clean[col].min():.2f}, {df_clean[col].max():.2f}]\\n\")\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "    # Categorical Analysis\n",
    "    categorical_cols = df_clean.select_dtypes(include=['object', 'category']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        f.write(\"CATEGORICAL VARIABLES SUMMARY\\n\")\n",
    "        f.write(\"-\"*40 + \"\\n\")\n",
    "        for col in categorical_cols[:3]:  # Top 3 categorical columns\n",
    "            f.write(f\"• {col}:\\n\")\n",
    "            f.write(f\"  - Unique Values: {df_clean[col].nunique()}\\n\")\n",
    "            f.write(f\"  - Most Frequent: '{df_clean[col].value_counts().index[0]}' ({df_clean[col].value_counts(normalize=True).iloc[0]*100:.1f}%)\\n\")\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "    # Recommendations\n",
    "    f.write(\"KEY INSIGHTS AND RECOMMENDATIONS\\n\")\n",
    "    f.write(\"-\"*45 + \"\\n\")\n",
    "    \n",
    "    # Data quality recommendations\n",
    "    if total_missing > 0:\n",
    "        f.write(\"• Data Quality: Consider implementing data validation rules to reduce missing values\\n\")\n",
    "    else:\n",
    "        f.write(\"• Data Quality: Excellent data completeness - maintain current data governance standards\\n\")\n",
    "    \n",
    "    # Performance recommendations\n",
    "    memory_mb = df_clean.memory_usage(deep=True).sum() / 1024**2\n",
    "    if memory_mb > 100:\n",
    "        f.write(\"• Performance: Consider data type optimization and partitioning for large datasets\\n\")\n",
    "    else:\n",
    "        f.write(\"• Performance: Current dataset size is manageable for in-memory analysis\\n\")\n",
    "    \n",
    "    # Business insights\n",
    "    if len(numerical_cols) > 0:\n",
    "        high_variation_cols = [col for col in numerical_cols if (df_clean[col].std() / df_clean[col].mean()) > 1]\n",
    "        if high_variation_cols:\n",
    "            f.write(f\"• Business Insights: High variation detected in {', '.join(high_variation_cols[:2])} - investigate underlying patterns\\n\")\n",
    "    \n",
    "    # Next steps\n",
    "    f.write(\"\\nRECOMMENDED NEXT STEPS\\n\")\n",
    "    f.write(\"-\"*30 + \"\\n\")\n",
    "    f.write(\"1. Implement automated data quality monitoring\\n\")\n",
    "    f.write(\"2. Establish baseline metrics for ongoing comparison\\n\")\n",
    "    f.write(\"3. Create scheduled analysis pipeline for regular insights\\n\")\n",
    "    f.write(\"4. Integrate findings with business intelligence dashboards\\n\")\n",
    "    f.write(\"5. Consider predictive modeling for key business metrics\\n\\n\")\n",
    "    \n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(\"Report generated by Alex Cognitive Architecture - Fishbowl POC\\n\")\n",
    "    f.write(\"Azure Enterprise Data Platform Analysis Framework\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(f\"   ✅ Executive summary exported to: {report_file}\")\n",
    "\n",
    "# 5. Create data dictionary\n",
    "print(\"📚 Creating data dictionary...\")\n",
    "dict_file = f\"{export_dir}/data_dictionary_{timestamp}.csv\"\n",
    "\n",
    "data_dict = []\n",
    "for col in df_clean.columns:\n",
    "    data_dict.append({\n",
    "        'Column_Name': col,\n",
    "        'Data_Type': str(df_clean[col].dtype),\n",
    "        'Non_Null_Count': df_clean[col].count(),\n",
    "        'Null_Count': df_clean[col].isnull().sum(),\n",
    "        'Unique_Values': df_clean[col].nunique(),\n",
    "        'Sample_Values': str(df_clean[col].dropna().head(3).tolist())\n",
    "    })\n",
    "\n",
    "data_dict_df = pd.DataFrame(data_dict)\n",
    "data_dict_df.to_csv(dict_file, index=False)\n",
    "print(f\"   ✅ Data dictionary exported to: {dict_file}\")\n",
    "\n",
    "# 6. Generate final summary\n",
    "print(f\"\\n🎉 ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"📊 Dataset Size: {len(df_clean):,} records × {len(df_clean.columns)} columns\")\n",
    "print(f\"🔍 Analysis Quality: {((len(df_clean) * len(df_clean.columns) - df_clean.isnull().sum().sum()) / (len(df_clean) * len(df_clean.columns)) * 100):.1f}% data completeness\")\n",
    "print(f\"💾 Files Exported: {len([f for f in os.listdir(export_dir) if timestamp in f])} files\")\n",
    "print(f\"📁 Export Location: {os.path.abspath(export_dir)}\")\n",
    "\n",
    "print(f\"\\n📋 Exported Files:\")\n",
    "print(\"-\" * 20)\n",
    "for file in os.listdir(export_dir):\n",
    "    if timestamp in file:\n",
    "        print(f\"   • {file}\")\n",
    "\n",
    "print(f\"\\n💡 Key Findings:\")\n",
    "print(\"-\" * 15)\n",
    "if len(numerical_cols) > 0:\n",
    "    print(f\"   • {len(numerical_cols)} numerical variables analyzed\")\n",
    "    print(f\"   • Correlation analysis completed for relationship mapping\")\n",
    "if len(categorical_cols) > 0:\n",
    "    print(f\"   • {len(categorical_cols)} categorical variables profiled\")\n",
    "print(f\"   • Data quality assessment: {'Excellent' if df_clean.isnull().sum().sum() < len(df_clean) * 0.05 else 'Good'}\")\n",
    "print(f\"   • Ready for advanced analytics and machine learning\")\n",
    "\n",
    "print(f\"\\n🚀 Next Steps: Review executive summary and implement recommended data governance practices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17368231",
   "metadata": {},
   "source": [
    "## 🔗 Adding External Storage Account as SQL Tables\n",
    "\n",
    "Methods to link `cpestaginglake` storage account as queryable tables in the SQL endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a162c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Create Lakehouse Shortcuts to External Storage Account\n",
    "def create_storage_shortcuts():\n",
    "    \"\"\"\n",
    "    Create shortcuts to link cpestaginglake storage account to the lakehouse\n",
    "    This makes external data appear as local tables in the SQL endpoint\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"🔗 Creating shortcuts to cpestaginglake storage account...\")\n",
    "        \n",
    "        # Method 1A: Using mssparkutils to create shortcuts\n",
    "        try:\n",
    "            from notebookutils import mssparkutils\n",
    "            \n",
    "            # Create shortcut to the external storage account\n",
    "            # This creates a logical link that appears in the lakehouse\n",
    "            shortcut_result = mssparkutils.lakehouse.create_shortcut(\n",
    "                source_path=\"abfss://your-container@cpestaginglake.dfs.core.windows.net/data/\",\n",
    "                target_path=\"/lakehouse/default/Files/cpestaginglake_data/\",\n",
    "                shortcut_name=\"cpestaginglake_shortcut\"\n",
    "            )\n",
    "            \n",
    "            print(\"✅ Shortcut created successfully via mssparkutils\")\n",
    "            print(f\"📁 Shortcut path: {shortcut_result}\")\n",
    "            \n",
    "        except Exception as shortcut_error:\n",
    "            print(f\"⚠️ mssparkutils shortcut failed: {str(shortcut_error)[:100]}...\")\n",
    "            \n",
    "            # Method 1B: Manual shortcut creation via SQL\n",
    "            try:\n",
    "                print(\"🔧 Attempting manual shortcut creation via SQL...\")\n",
    "                \n",
    "                # Create external location reference\n",
    "                create_shortcut_sql = \"\"\"\n",
    "                CREATE OR REPLACE SHORTCUT cpestaginglake_data\n",
    "                IN '/lakehouse/default/Files/'\n",
    "                FROM 'abfss://your-container@cpestaginglake.dfs.core.windows.net/data/'\n",
    "                \"\"\"\n",
    "                \n",
    "                spark.sql(create_shortcut_sql)\n",
    "                print(\"✅ Manual shortcut created via SQL\")\n",
    "                \n",
    "            except Exception as manual_error:\n",
    "                print(f\"🔧 Manual shortcut creation failed: {str(manual_error)[:100]}...\")\n",
    "                raise manual_error\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Shortcut creation failed: {str(e)}\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Method 2: Create External Tables for Direct SQL Access\n",
    "def create_external_tables():\n",
    "    \"\"\"\n",
    "    Create external tables that directly reference cpestaginglake storage\n",
    "    This enables direct SQL querying without copying data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"📊 Creating external tables for cpestaginglake...\")\n",
    "        \n",
    "        # Example external table creation for different data formats\n",
    "        external_tables = [\n",
    "            {\n",
    "                \"table_name\": \"cpestaging_accounts\",\n",
    "                \"location\": \"abfss://accounts@cpestaginglake.dfs.core.windows.net/\",\n",
    "                \"format\": \"PARQUET\"\n",
    "            },\n",
    "            {\n",
    "                \"table_name\": \"cpestaging_transactions\", \n",
    "                \"location\": \"abfss://transactions@cpestaginglake.dfs.core.windows.net/\",\n",
    "                \"format\": \"DELTA\"\n",
    "            },\n",
    "            {\n",
    "                \"table_name\": \"cpestaging_customers\",\n",
    "                \"location\": \"abfss://customers@cpestaginglake.dfs.core.windows.net/\",\n",
    "                \"format\": \"CSV\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        for table_config in external_tables:\n",
    "            try:\n",
    "                print(f\"📋 Creating external table: {table_config['table_name']}\")\n",
    "                \n",
    "                if table_config['format'] == 'PARQUET':\n",
    "                    create_table_sql = f\"\"\"\n",
    "                    CREATE TABLE IF NOT EXISTS {table_config['table_name']}\n",
    "                    USING PARQUET\n",
    "                    LOCATION '{table_config['location']}'\n",
    "                    \"\"\"\n",
    "                \n",
    "                elif table_config['format'] == 'DELTA':\n",
    "                    create_table_sql = f\"\"\"\n",
    "                    CREATE TABLE IF NOT EXISTS {table_config['table_name']}\n",
    "                    USING DELTA\n",
    "                    LOCATION '{table_config['location']}'\n",
    "                    \"\"\"\n",
    "                \n",
    "                elif table_config['format'] == 'CSV':\n",
    "                    create_table_sql = f\"\"\"\n",
    "                    CREATE TABLE IF NOT EXISTS {table_config['table_name']}\n",
    "                    USING CSV\n",
    "                    OPTIONS (\n",
    "                        path '{table_config['location']}',\n",
    "                        header 'true',\n",
    "                        inferSchema 'true'\n",
    "                    )\n",
    "                    \"\"\"\n",
    "                \n",
    "                spark.sql(create_table_sql)\n",
    "                print(f\"✅ External table created: {table_config['table_name']}\")\n",
    "                \n",
    "            except Exception as table_error:\n",
    "                print(f\"⚠️ Failed to create {table_config['table_name']}: {str(table_error)[:100]}...\")\n",
    "                continue\n",
    "        \n",
    "        print(\"📊 External table creation process completed\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ External table creation failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Method 3: Discover and Auto-Create Tables from Storage Account\n",
    "def discover_and_create_tables():\n",
    "    \"\"\"\n",
    "    Automatically discover data in cpestaginglake and create appropriate tables\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"🔍 Discovering data structure in cpestaginglake...\")\n",
    "        \n",
    "        # List containers and folders in the storage account\n",
    "        try:\n",
    "            from notebookutils import mssparkutils\n",
    "            \n",
    "            # List top-level containers\n",
    "            containers = mssparkutils.fs.ls(\"abfss://cpestaginglake.dfs.core.windows.net/\")\n",
    "            print(f\"📁 Found {len(containers)} containers:\")\n",
    "            \n",
    "            for container in containers:\n",
    "                print(f\"   📦 {container.name}\")\n",
    "                \n",
    "                # Explore each container for data files\n",
    "                try:\n",
    "                    container_path = f\"abfss://{container.name}@cpestaginglake.dfs.core.windows.net/\"\n",
    "                    files = mssparkutils.fs.ls(container_path)\n",
    "                    \n",
    "                    print(f\"      📄 {len(files)} items in {container.name}\")\n",
    "                    \n",
    "                    # Look for common data file patterns\n",
    "                    data_files = [f for f in files if any(f.name.endswith(ext) for ext in ['.parquet', '.csv', '.json', '.delta'])]\n",
    "                    \n",
    "                    if data_files:\n",
    "                        print(f\"      💾 {len(data_files)} data files found\")\n",
    "                        \n",
    "                        # Auto-create table for this container\n",
    "                        table_name = f\"cpestaging_{container.name.replace('-', '_')}\"\n",
    "                        \n",
    "                        # Detect format from first data file\n",
    "                        first_file = data_files[0]\n",
    "                        if first_file.name.endswith('.parquet'):\n",
    "                            create_sql = f\"\"\"\n",
    "                            CREATE TABLE IF NOT EXISTS {table_name}\n",
    "                            USING PARQUET\n",
    "                            LOCATION '{container_path}'\n",
    "                            \"\"\"\n",
    "                        elif first_file.name.endswith('.csv'):\n",
    "                            create_sql = f\"\"\"\n",
    "                            CREATE TABLE IF NOT EXISTS {table_name}\n",
    "                            USING CSV\n",
    "                            OPTIONS (\n",
    "                                path '{container_path}',\n",
    "                                header 'true',\n",
    "                                inferSchema 'true'\n",
    "                            )\n",
    "                            \"\"\"\n",
    "                        else:\n",
    "                            continue\n",
    "                        \n",
    "                        spark.sql(create_sql)\n",
    "                        print(f\"      ✅ Created table: {table_name}\")\n",
    "                        \n",
    "                except Exception as container_error:\n",
    "                    print(f\"      ⚠️ Could not process container {container.name}: {str(container_error)[:50]}...\")\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as discovery_error:\n",
    "            print(f\"🔍 Discovery failed: {str(discovery_error)[:100]}...\")\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Auto-discovery failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Method 4: Test Access to External Tables\n",
    "def test_external_table_access():\n",
    "    \"\"\"\n",
    "    Test that the external tables are accessible via SQL endpoint\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"🧪 Testing access to external tables...\")\n",
    "        \n",
    "        # List all available tables\n",
    "        tables = spark.sql(\"SHOW TABLES\").collect()\n",
    "        external_tables = [t for t in tables if t.tableName.startswith('cpestaging_')]\n",
    "        \n",
    "        print(f\"📋 Found {len(external_tables)} cpestaging tables:\")\n",
    "        \n",
    "        for table in external_tables:\n",
    "            try:\n",
    "                print(f\"   📊 Testing {table.tableName}...\")\n",
    "                \n",
    "                # Get row count\n",
    "                count_result = spark.sql(f\"SELECT COUNT(*) as row_count FROM {table.tableName}\").collect()[0]\n",
    "                row_count = count_result['row_count']\n",
    "                \n",
    "                # Get sample data\n",
    "                sample = spark.sql(f\"SELECT * FROM {table.tableName} LIMIT 5\").toPandas()\n",
    "                \n",
    "                print(f\"      ✅ {row_count:,} rows, {len(sample.columns)} columns\")\n",
    "                print(f\"      📝 Columns: {list(sample.columns)[:5]}...\")  # Show first 5 columns\n",
    "                \n",
    "            except Exception as test_error:\n",
    "                print(f\"      ❌ Access failed: {str(test_error)[:50]}...\")\n",
    "                continue\n",
    "                \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Table access test failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Execute the setup process\n",
    "print(\"🚀 Setting up cpestaginglake storage account access...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Create shortcuts (if supported)\n",
    "print(\"\\n📎 Step 1: Creating shortcuts...\")\n",
    "shortcuts_created = create_storage_shortcuts()\n",
    "\n",
    "# Step 2: Create external tables\n",
    "print(\"\\n📊 Step 2: Creating external tables...\")\n",
    "tables_created = create_external_tables()\n",
    "\n",
    "# Step 3: Auto-discover and create tables\n",
    "print(\"\\n🔍 Step 3: Auto-discovering data...\")\n",
    "discovery_completed = discover_and_create_tables()\n",
    "\n",
    "# Step 4: Test access\n",
    "print(\"\\n🧪 Step 4: Testing table access...\")\n",
    "access_tested = test_external_table_access()\n",
    "\n",
    "print(\"\\n📈 Setup Summary:\")\n",
    "print(f\"   Shortcuts: {'✅' if shortcuts_created else '❌'}\")\n",
    "print(f\"   External Tables: {'✅' if tables_created else '❌'}\")\n",
    "print(f\"   Auto-Discovery: {'✅' if discovery_completed else '❌'}\")\n",
    "print(f\"   Access Test: {'✅' if access_tested else '❌'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9452c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for cpestaginglake Storage Account\n",
    "# Update these settings to match your specific storage account structure\n",
    "\n",
    "STORAGE_CONFIG = {\n",
    "    \"account_name\": \"cpestaginglake\",\n",
    "    \"containers\": [\n",
    "        {\n",
    "            \"name\": \"data\",  # Replace with actual container name\n",
    "            \"table_name\": \"cpestaging_data\",\n",
    "            \"format\": \"PARQUET\"  # or \"DELTA\", \"CSV\", \"JSON\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"accounts\", \n",
    "            \"table_name\": \"cpestaging_accounts\",\n",
    "            \"format\": \"PARQUET\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"transactions\",\n",
    "            \"table_name\": \"cpestaging_transactions\", \n",
    "            \"format\": \"DELTA\"\n",
    "        }\n",
    "    ],\n",
    "    \"base_path\": \"abfss://{container}@cpestaginglake.dfs.core.windows.net/\"\n",
    "}\n",
    "\n",
    "# Quick function to create external table for cpestaginglake\n",
    "def create_cpestaging_table(container_name, table_name, data_format=\"PARQUET\"):\n",
    "    \"\"\"\n",
    "    Quick function to create external table for a specific container in cpestaginglake\n",
    "    \n",
    "    Args:\n",
    "        container_name (str): Name of the container in cpestaginglake\n",
    "        table_name (str): Name for the SQL table\n",
    "        data_format (str): Data format - PARQUET, DELTA, CSV, JSON\n",
    "    \"\"\"\n",
    "    try:\n",
    "        location = f\"abfss://{container_name}@cpestaginglake.dfs.core.windows.net/\"\n",
    "        \n",
    "        if data_format.upper() == \"PARQUET\":\n",
    "            sql_command = f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {table_name}\n",
    "            USING PARQUET\n",
    "            LOCATION '{location}'\n",
    "            \"\"\"\n",
    "        elif data_format.upper() == \"DELTA\":\n",
    "            sql_command = f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {table_name}\n",
    "            USING DELTA\n",
    "            LOCATION '{location}'\n",
    "            \"\"\"\n",
    "        elif data_format.upper() == \"CSV\":\n",
    "            sql_command = f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {table_name}\n",
    "            USING CSV\n",
    "            OPTIONS (\n",
    "                path '{location}',\n",
    "                header 'true',\n",
    "                inferSchema 'true'\n",
    "            )\n",
    "            \"\"\"\n",
    "        elif data_format.upper() == \"JSON\":\n",
    "            sql_command = f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {table_name}\n",
    "            USING JSON\n",
    "            LOCATION '{location}'\n",
    "            \"\"\"\n",
    "        \n",
    "        print(f\"🔄 Creating table {table_name} from {container_name}...\")\n",
    "        spark.sql(sql_command)\n",
    "        \n",
    "        # Test the table\n",
    "        row_count = spark.sql(f\"SELECT COUNT(*) as count FROM {table_name}\").collect()[0]['count']\n",
    "        print(f\"✅ Table {table_name} created successfully with {row_count:,} rows\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to create table {table_name}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Examples of usage:\n",
    "print(\"💡 Quick Setup Examples:\")\n",
    "print(\"   create_cpestaging_table('your-container', 'cpestaging_yourdata', 'PARQUET')\")\n",
    "print(\"   create_cpestaging_table('accounts', 'cpestaging_accounts', 'DELTA')\")\n",
    "print(\"   create_cpestaging_table('logs', 'cpestaging_logs', 'CSV')\")\n",
    "print(\"\\n🔧 Update STORAGE_CONFIG above with your actual container names and formats\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
