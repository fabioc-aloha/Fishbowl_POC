{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5c006f9",
   "metadata": {},
   "source": [
    "# OneLake Data Analysis: dimcyallaccounts Table\n",
    "\n",
    "**Project**: Fishbowl POC - Enterprise Data Platform Analysis  \n",
    "**Date**: August 7, 2025  \n",
    "**Data Source**: Microsoft Fabric OneLake - CPE Staging Lake  \n",
    "**Table**: dimcyallaccounts  \n",
    "\n",
    "## Objective\n",
    "Comprehensive analysis of the dimcyallaccounts table from OneLake to understand:\n",
    "- Data structure and quality\n",
    "- Key business insights and patterns\n",
    "- Statistical distributions and correlations\n",
    "- Recommendations for data utilization\n",
    "\n",
    "## Data Source Details\n",
    "- **OneLake URL**: `https://msit-onelake.dfs.fabric.microsoft.com/Fishbowl_POC/FishbowlOneLake.Lakehouse/Files/synapse/dimcyallaccounts`\n",
    "- **Environment**: CPE Staging Lake\n",
    "- **Analysis Framework**: Python with Azure SDK integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d67d01",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Setting up the analysis environment with essential libraries for data processing, visualization, and Azure integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee9688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data analysis libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Azure SDK for OneLake integration (using managed identity - Azure best practice)\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "import requests\n",
    "import io\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"üìä Pandas version: {pd.__version__}\")\n",
    "print(f\"üî¢ NumPy version: {np.__version__}\")\n",
    "print(f\"üìà Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"üé® Seaborn version: {sns.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62bcd56",
   "metadata": {},
   "source": [
    "## 2. Connect to OneLake and Load Data\n",
    "\n",
    "Establishing secure connection to Microsoft Fabric OneLake using Azure managed identity and loading the dimcyallaccounts table data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c05e646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneLake connection configuration\n",
    "ONELAKE_URL = \"https://msit-onelake.dfs.fabric.microsoft.com/Fishbowl_POC/FishbowlOneLake.Lakehouse/Files/synapse/dimcyallaccounts\"\n",
    "ACCOUNT_NAME = \"msit-onelake\"\n",
    "FILESYSTEM_NAME = \"Fishbowl_POC\"\n",
    "FILE_PATH = \"FishbowlOneLake.Lakehouse/Files/synapse/dimcyallaccounts\"\n",
    "\n",
    "def load_onelake_data():\n",
    "    \"\"\"\n",
    "    Load data from OneLake using Azure managed identity (best practice for security)\n",
    "    Implements retry logic and proper error handling\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"üîê Authenticating with Azure using managed identity...\")\n",
    "        \n",
    "        # Use DefaultAzureCredential for secure authentication (Azure best practice)\n",
    "        credential = DefaultAzureCredential()\n",
    "        \n",
    "        # Create DataLake service client\n",
    "        service_client = DataLakeServiceClient(\n",
    "            account_url=f\"https://{ACCOUNT_NAME}.dfs.core.windows.net\",\n",
    "            credential=credential\n",
    "        )\n",
    "        \n",
    "        print(\"üîó Connecting to OneLake filesystem...\")\n",
    "        filesystem_client = service_client.get_file_system_client(FILESYSTEM_NAME)\n",
    "        \n",
    "        print(\"üìÅ Accessing data file...\")\n",
    "        file_client = filesystem_client.get_file_client(FILE_PATH)\n",
    "        \n",
    "        print(\"‚¨áÔ∏è Downloading data...\")\n",
    "        download = file_client.download_file()\n",
    "        downloaded_bytes = download.readall()\n",
    "        \n",
    "        # Detect file format and load accordingly\n",
    "        if FILE_PATH.endswith('.parquet'):\n",
    "            df = pd.read_parquet(io.BytesIO(downloaded_bytes))\n",
    "        elif FILE_PATH.endswith('.csv'):\n",
    "            df = pd.read_csv(io.BytesIO(downloaded_bytes))\n",
    "        elif FILE_PATH.endswith('.json'):\n",
    "            df = pd.read_json(io.BytesIO(downloaded_bytes))\n",
    "        else:\n",
    "            # Try to detect format from content\n",
    "            try:\n",
    "                df = pd.read_parquet(io.BytesIO(downloaded_bytes))\n",
    "                print(\"üìÑ Detected Parquet format\")\n",
    "            except:\n",
    "                try:\n",
    "                    df = pd.read_csv(io.BytesIO(downloaded_bytes))\n",
    "                    print(\"üìÑ Detected CSV format\")\n",
    "                except:\n",
    "                    df = pd.read_json(io.BytesIO(downloaded_bytes))\n",
    "                    print(\"üìÑ Detected JSON format\")\n",
    "        \n",
    "        print(f\"‚úÖ Data loaded successfully: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading data: {str(e)}\")\n",
    "        print(\"üí° Fallback: Creating sample data for demonstration...\")\n",
    "        \n",
    "        # Create sample data structure for analysis demonstration\n",
    "        np.random.seed(42)\n",
    "        sample_data = {\n",
    "            'account_id': range(1, 1001),\n",
    "            'account_name': [f'Account_{i:04d}' for i in range(1, 1001)],\n",
    "            'account_type': np.random.choice(['Premium', 'Standard', 'Basic'], 1000),\n",
    "            'created_date': pd.date_range('2020-01-01', periods=1000, freq='D'),\n",
    "            'last_activity': pd.date_range('2024-01-01', periods=1000, freq='H'),\n",
    "            'balance': np.random.normal(10000, 5000, 1000).round(2),\n",
    "            'transaction_count': np.random.poisson(50, 1000),\n",
    "            'region': np.random.choice(['North', 'South', 'East', 'West'], 1000),\n",
    "            'status': np.random.choice(['Active', 'Inactive', 'Suspended'], 1000, p=[0.7, 0.2, 0.1])\n",
    "        }\n",
    "        df = pd.DataFrame(sample_data)\n",
    "        print(\"üìä Sample data created for analysis demonstration\")\n",
    "        return df\n",
    "\n",
    "# Load the data\n",
    "print(\"üöÄ Starting OneLake data loading process...\")\n",
    "df_accounts = load_onelake_data()\n",
    "\n",
    "# Display basic information\n",
    "print(f\"\\nüìà Dataset Overview:\")\n",
    "print(f\"Shape: {df_accounts.shape}\")\n",
    "print(f\"Memory usage: {df_accounts.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af136b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intelligent Column Analysis and Filtering\n",
    "print(\"üß† INTELLIGENT COLUMN ANALYSIS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "def categorize_columns(df):\n",
    "    \"\"\"\n",
    "    Intelligently categorize columns based on their names and data types\n",
    "    for appropriate analysis methods\n",
    "    \"\"\"\n",
    "    column_categories = {\n",
    "        'id_columns': [],           # Columns ending with 'id' - to be excluded from analysis\n",
    "        'numerical_measures': [],   # Numerical columns for statistical analysis\n",
    "        'categorical_features': [], # Categorical columns for frequency analysis  \n",
    "        'date_columns': [],        # Date columns for temporal analysis\n",
    "        'text_columns': [],        # Text columns for content analysis\n",
    "        'other_columns': []        # Columns that don't fit other categories\n",
    "    }\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        dtype = str(df[col].dtype)\n",
    "        \n",
    "        # Skip ID columns (any column ending with 'id')\n",
    "        if col_lower.endswith('id'):\n",
    "            column_categories['id_columns'].append(col)\n",
    "            continue\n",
    "            \n",
    "        # Categorize by data type and name patterns\n",
    "        if 'datetime' in dtype or 'date' in col_lower:\n",
    "            column_categories['date_columns'].append(col)\n",
    "        elif dtype in ['int64', 'int32', 'float64', 'float32'] and not col_lower.endswith('id'):\n",
    "            # Only include numerical columns that aren't IDs\n",
    "            column_categories['numerical_measures'].append(col)\n",
    "        elif dtype in ['object', 'category'] or 'string' in dtype:\n",
    "            # Check if it's likely categorical vs text\n",
    "            unique_ratio = df[col].nunique() / len(df)\n",
    "            if unique_ratio < 0.5:  # Less than 50% unique values = categorical\n",
    "                column_categories['categorical_features'].append(col)\n",
    "            else:\n",
    "                column_categories['text_columns'].append(col)\n",
    "        else:\n",
    "            column_categories['other_columns'].append(col)\n",
    "    \n",
    "    return column_categories\n",
    "\n",
    "# Analyze and categorize columns\n",
    "column_info = categorize_columns(df_accounts)\n",
    "\n",
    "print(\"üìä Column Categorization:\")\n",
    "print(\"-\" * 30)\n",
    "for category, columns in column_info.items():\n",
    "    if columns:\n",
    "        print(f\"üè∑Ô∏è  {category.upper().replace('_', ' ')}: {len(columns)} columns\")\n",
    "        for col in columns:\n",
    "            print(f\"   ‚Ä¢ {col} ({str(df_accounts[col].dtype)})\")\n",
    "        print()\n",
    "\n",
    "# Create filtered datasets for analysis\n",
    "print(\"üéØ Creating filtered datasets for meaningful analysis...\")\n",
    "\n",
    "# Exclude ID columns from analysis\n",
    "analysis_columns = [col for col in df_accounts.columns if not col.lower().endswith('id')]\n",
    "df_for_analysis = df_accounts[analysis_columns].copy()\n",
    "\n",
    "print(f\"‚úÖ Original dataset: {df_accounts.shape[1]} columns\")\n",
    "print(f\"‚úÖ Analysis dataset: {df_for_analysis.shape[1]} columns\")\n",
    "print(f\"üìù Excluded ID columns: {column_info['id_columns']}\")\n",
    "\n",
    "# Create separate datasets for different types of analysis\n",
    "numerical_cols = column_info['numerical_measures']\n",
    "categorical_cols = column_info['categorical_features'] \n",
    "date_cols = column_info['date_columns']\n",
    "\n",
    "print(f\"\\nüî¢ Numerical columns for statistical analysis: {numerical_cols}\")\n",
    "print(f\"üè∑Ô∏è  Categorical columns for frequency analysis: {categorical_cols}\")\n",
    "print(f\"üìÖ Date columns for temporal analysis: {date_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d171929d",
   "metadata": {},
   "source": [
    "## 3. Data Exploration and Overview\n",
    "\n",
    "Examining the structure, data types, and basic characteristics of the dimcyallaccounts dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a441311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Dataset Structure Analysis (Excluding ID Columns)\n",
    "print(\"üîç ENHANCED DATASET STRUCTURE ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"üìä Original Shape: {df_accounts.shape[0]:,} rows √ó {df_accounts.shape[1]} columns\")\n",
    "print(f\"\udcca Analysis Shape: {df_for_analysis.shape[0]:,} rows √ó {df_for_analysis.shape[1]} columns\")\n",
    "print(f\"\ud83düíæ Memory Usage: {df_for_analysis.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(f\"\\nüìã Analysis-Ready Column Information:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"üö´ Excluded ID columns:\", column_info['id_columns'])\n",
    "print(\"‚úÖ Including meaningful columns for analysis\")\n",
    "print()\n",
    "\n",
    "# Show info for analysis dataset (without ID columns)\n",
    "df_for_analysis.info()\n",
    "\n",
    "print(f\"\\nüéØ Data Types Summary (Analysis Dataset):\")\n",
    "print(\"-\" * 35)\n",
    "print(df_for_analysis.dtypes.value_counts())\n",
    "\n",
    "print(f\"\\nüëÄ First 5 Rows (Analysis Dataset):\")\n",
    "print(\"-\" * 30)\n",
    "display(df_for_analysis.head())\n",
    "\n",
    "print(f\"\\nüìà Intelligent Statistics by Data Type:\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Numerical Analysis (excluding meaningless operations)\n",
    "if numerical_cols:\n",
    "    print(\"üî¢ NUMERICAL MEASURES ANALYSIS:\")\n",
    "    print(\"-\" * 35)\n",
    "    numerical_stats = df_for_analysis[numerical_cols].describe()\n",
    "    display(numerical_stats.round(2))\n",
    "    \n",
    "    # Additional meaningful numerical insights\n",
    "    for col in numerical_cols:\n",
    "        print(f\"\\n\udcca {col.upper()} Insights:\")\n",
    "        values = df_for_analysis[col]\n",
    "        print(f\"   ‚Ä¢ Range: {values.min():.2f} to {values.max():.2f}\")\n",
    "        print(f\"   ‚Ä¢ Spread: {values.std():.2f} (CV: {(values.std()/values.mean()*100):.1f}%)\")\n",
    "        print(f\"   ‚Ä¢ Outliers: {len(values[abs(values - values.mean()) > 2*values.std()])} records\")\n",
    "\n",
    "# Categorical Analysis  \n",
    "if categorical_cols:\n",
    "    print(f\"\\nüè∑Ô∏è  CATEGORICAL FEATURES ANALYSIS:\")\n",
    "    print(\"-\" * 40)\n",
    "    for col in categorical_cols:\n",
    "        print(f\"\\nüìä {col.upper()} Distribution:\")\n",
    "        value_counts = df_for_analysis[col].value_counts()\n",
    "        value_percentages = (value_counts / len(df_for_analysis) * 100).round(1)\n",
    "        \n",
    "        for idx, (value, count) in enumerate(value_counts.head().items()):\n",
    "            print(f\"   ‚Ä¢ {value}: {count:,} ({value_percentages.iloc[idx]}%)\")\n",
    "\n",
    "# Date Analysis (appropriate temporal insights)\n",
    "if date_cols:\n",
    "    print(f\"\\nüìÖ TEMPORAL ANALYSIS:\")\n",
    "    print(\"-\" * 25)\n",
    "    for col in date_cols:\n",
    "        print(f\"\\nüìä {col.upper()} Temporal Insights:\")\n",
    "        date_series = df_for_analysis[col]\n",
    "        print(f\"   ‚Ä¢ Date Range: {date_series.min().strftime('%Y-%m-%d')} to {date_series.max().strftime('%Y-%m-%d')}\")\n",
    "        print(f\"   ‚Ä¢ Time Span: {(date_series.max() - date_series.min()).days:,} days\")\n",
    "        \n",
    "        # Year distribution (meaningful for dates)\n",
    "        if hasattr(date_series.dt, 'year'):\n",
    "            year_dist = date_series.dt.year.value_counts().sort_index()\n",
    "            print(f\"   ‚Ä¢ Year Distribution: {dict(year_dist.head(3))}\")\n",
    "        \n",
    "        # Month distribution (meaningful for business patterns)\n",
    "        if hasattr(date_series.dt, 'month'):\n",
    "            month_dist = date_series.dt.month.value_counts().sort_index()\n",
    "            popular_months = month_dist.head(3)\n",
    "            print(f\"   ‚Ä¢ Most Active Months: {dict(popular_months)}\")\n",
    "\n",
    "print(f\"\\n‚úÖ ANALYSIS-READY SUMMARY:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"‚Ä¢ Total records: {len(df_for_analysis):,}\")\n",
    "print(f\"‚Ä¢ Numerical measures: {len(numerical_cols)} columns\")\n",
    "print(f\"‚Ä¢ Categorical features: {len(categorical_cols)} columns\") \n",
    "print(f\"‚Ä¢ Date fields: {len(date_cols)} columns\")\n",
    "print(f\"‚Ä¢ ID columns excluded: {len(column_info['id_columns'])} columns\")\n",
    "print(f\"‚Ä¢ Ready for meaningful analysis: ‚úÖ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de74b675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality Assessment\n",
    "print(\"üîé DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Missing values analysis\n",
    "print(\"‚ùì Missing Values Analysis:\")\n",
    "print(\"-\" * 30)\n",
    "missing_data = df_accounts.isnull().sum()\n",
    "missing_percent = (missing_data / len(df_accounts)) * 100\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing_Count': missing_data,\n",
    "    'Missing_Percentage': missing_percent.round(2)\n",
    "}).sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "print(missing_summary[missing_summary['Missing_Count'] > 0])\n",
    "\n",
    "# Duplicate records check\n",
    "duplicates = df_accounts.duplicated().sum()\n",
    "print(f\"\\nüîÑ Duplicate Records: {duplicates:,}\")\n",
    "\n",
    "# Unique values per column\n",
    "print(f\"\\nüéØ Unique Values per Column:\")\n",
    "print(\"-\" * 35)\n",
    "for col in df_accounts.columns:\n",
    "    unique_count = df_accounts[col].nunique()\n",
    "    unique_percent = (unique_count / len(df_accounts)) * 100\n",
    "    print(f\"{col:<20}: {unique_count:>8,} ({unique_percent:>5.1f}%)\")\n",
    "\n",
    "# Data type consistency check\n",
    "print(f\"\\n‚úÖ Data Quality Summary:\")\n",
    "print(\"-\" * 25)\n",
    "print(f\"‚Ä¢ Total records: {len(df_accounts):,}\")\n",
    "print(f\"‚Ä¢ Complete records: {len(df_accounts) - df_accounts.isnull().any(axis=1).sum():,}\")\n",
    "print(f\"‚Ä¢ Data completeness: {((len(df_accounts) - df_accounts.isnull().any(axis=1).sum()) / len(df_accounts) * 100):.1f}%\")\n",
    "print(f\"‚Ä¢ Duplicate records: {duplicates:,}\")\n",
    "print(f\"‚Ä¢ Data uniqueness: {((len(df_accounts) - duplicates) / len(df_accounts) * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3d9495",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning and Preprocessing\n",
    "\n",
    "Handling missing values, removing duplicates, and preparing data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cc36d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intelligent Data Cleaning and Preprocessing (Data Type Aware)\n",
    "print(\"üßπ INTELLIGENT DATA CLEANING AND PREPROCESSING\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Create a cleaned copy of the analysis dataset (excluding ID columns)\n",
    "df_clean = df_for_analysis.copy()\n",
    "initial_rows = len(df_clean)\n",
    "\n",
    "print(f\"üìä Initial analysis dataset size: {initial_rows:,} rows √ó {df_clean.shape[1]} columns\")\n",
    "print(f\"üö´ ID columns excluded from analysis: {column_info['id_columns']}\")\n",
    "\n",
    "# 1. Remove duplicate records\n",
    "duplicates_removed = df_clean.duplicated().sum()\n",
    "if duplicates_removed > 0:\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "    print(f\"üîÑ Removed {duplicates_removed:,} duplicate records\")\n",
    "else:\n",
    "    print(f\"‚úÖ No duplicate records found\")\n",
    "\n",
    "# 2. Intelligent missing value handling by data type\n",
    "missing_before = df_clean.isnull().sum().sum()\n",
    "print(f\"\\n‚ùì Missing values before cleaning: {missing_before:,}\")\n",
    "\n",
    "if missing_before > 0:\n",
    "    print(f\"\\nüéØ APPLYING DATA TYPE-SPECIFIC CLEANING:\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    # Handle missing values by column category\n",
    "    for column in df_clean.columns:\n",
    "        missing_count = df_clean[column].isnull().sum()\n",
    "        if missing_count > 0:\n",
    "            missing_percent = (missing_count / len(df_clean)) * 100\n",
    "            print(f\"\\nüìä {column}: {missing_count:,} missing ({missing_percent:.1f}%)\")\n",
    "            \n",
    "            # Apply cleaning strategy based on column category and missing percentage\n",
    "            if missing_percent > 50:\n",
    "                print(f\"     ‚Üí Dropping column {column} (>50% missing)\")\n",
    "                df_clean = df_clean.drop(columns=[column])\n",
    "                # Remove from our category tracking\n",
    "                for cat_list in column_info.values():\n",
    "                    if column in cat_list:\n",
    "                        cat_list.remove(column)\n",
    "                        \n",
    "            elif column in numerical_cols:\n",
    "                # Numerical measures: use median (robust to outliers)\n",
    "                median_value = df_clean[column].median()\n",
    "                df_clean[column] = df_clean[column].fillna(median_value)\n",
    "                print(f\"     ‚Üí Filled with median: {median_value:.2f}\")\n",
    "                \n",
    "            elif column in categorical_cols:\n",
    "                # Categorical features: use mode or create 'Unknown' category\n",
    "                mode_value = df_clean[column].mode()\n",
    "                if len(mode_value) > 0:\n",
    "                    fill_value = mode_value[0]\n",
    "                    df_clean[column] = df_clean[column].fillna(fill_value)\n",
    "                    print(f\"     ‚Üí Filled with mode: '{fill_value}'\")\n",
    "                else:\n",
    "                    df_clean[column] = df_clean[column].fillna('Unknown')\n",
    "                    print(f\"     ‚Üí Filled with: 'Unknown'\")\n",
    "                    \n",
    "            elif column in date_cols:\n",
    "                # Date columns: strategy based on missing percentage and business logic\n",
    "                if missing_percent < 5:\n",
    "                    # Small percentage: drop rows to maintain data quality\n",
    "                    df_clean = df_clean.dropna(subset=[column])\n",
    "                    print(f\"     ‚Üí Dropped rows with missing dates (<5% missing)\")\n",
    "                else:\n",
    "                    # Higher percentage: use median date\n",
    "                    median_date = df_clean[column].median()\n",
    "                    df_clean[column] = df_clean[column].fillna(median_date)\n",
    "                    print(f\"     ‚Üí Filled with median date: {median_date}\")\n",
    "            else:\n",
    "                # Other columns: conservative approach\n",
    "                if missing_percent < 10:\n",
    "                    df_clean = df_clean.dropna(subset=[column])\n",
    "                    print(f\"     ‚Üí Dropped rows with missing values\")\n",
    "                else:\n",
    "                    df_clean[column] = df_clean[column].fillna('Unknown')\n",
    "                    print(f\"     ‚Üí Filled with: 'Unknown'\")\n",
    "\n",
    "# 3. Data type optimization for performance\n",
    "print(f\"\\nüîß OPTIMIZING DATA TYPES FOR PERFORMANCE:\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "memory_before = df_clean.memory_usage(deep=True).sum()\n",
    "\n",
    "for column in df_clean.columns:\n",
    "    dtype_before = str(df_clean[column].dtype)\n",
    "    \n",
    "    if column in categorical_cols and df_clean[column].dtype == 'object':\n",
    "        # Convert categorical strings to category type for memory efficiency\n",
    "        unique_ratio = df_clean[column].nunique() / len(df_clean)\n",
    "        if unique_ratio < 0.5:  # Less than 50% unique values\n",
    "            df_clean[column] = df_clean[column].astype('category')\n",
    "            print(f\"   ‚Ä¢ {column}: object ‚Üí category (memory optimization)\")\n",
    "            \n",
    "    elif column in numerical_cols:\n",
    "        # Optimize numerical types\n",
    "        if df_clean[column].dtype == 'float64':\n",
    "            # Check if we can use float32\n",
    "            if df_clean[column].min() >= np.finfo(np.float32).min and df_clean[column].max() <= np.finfo(np.float32).max:\n",
    "                df_clean[column] = df_clean[column].astype('float32')\n",
    "                print(f\"   ‚Ä¢ {column}: float64 ‚Üí float32 (memory optimization)\")\n",
    "        elif df_clean[column].dtype == 'int64':\n",
    "            # Check if we can use smaller integer types\n",
    "            if df_clean[column].min() >= np.iinfo(np.int32).min and df_clean[column].max() <= np.iinfo(np.int32).max:\n",
    "                df_clean[column] = df_clean[column].astype('int32')\n",
    "                print(f\"   ‚Ä¢ {column}: int64 ‚Üí int32 (memory optimization)\")\n",
    "\n",
    "# 4. Create business-relevant derived features\n",
    "print(f\"\\nüéØ CREATING BUSINESS-RELEVANT DERIVED FEATURES:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Date-based features (only meaningful business features)\n",
    "for col in date_cols:\n",
    "    if col in df_clean.columns:  # Check if column still exists after cleaning\n",
    "        # Business calendar features\n",
    "        df_clean[f'{col}_year'] = df_clean[col].dt.year\n",
    "        df_clean[f'{col}_quarter'] = df_clean[col].dt.quarter\n",
    "        df_clean[f'{col}_month'] = df_clean[col].dt.month\n",
    "        df_clean[f'{col}_is_weekend'] = df_clean[col].dt.dayofweek >= 5\n",
    "        \n",
    "        # Business age calculations\n",
    "        if 'created' in col.lower() or 'start' in col.lower():\n",
    "            reference_date = df_clean[col].max()\n",
    "            df_clean[f'{col}_age_days'] = (reference_date - df_clean[col]).dt.days\n",
    "            print(f\"   ‚Ä¢ Created age calculation for {col}\")\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Created business calendar features for {col}\")\n",
    "\n",
    "# Update column categories after cleaning\n",
    "column_info_clean = categorize_columns(df_clean)\n",
    "numerical_cols_clean = column_info_clean['numerical_measures']\n",
    "categorical_cols_clean = column_info_clean['categorical_features']\n",
    "date_cols_clean = column_info_clean['date_columns']\n",
    "\n",
    "# 5. Outlier analysis for numerical columns\n",
    "print(f\"\\nüîç OUTLIER ANALYSIS FOR BUSINESS INSIGHTS:\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "outlier_summary = []\n",
    "for col in numerical_cols_clean:\n",
    "    if col in df_clean.columns:\n",
    "        Q1 = df_clean[col].quantile(0.25)\n",
    "        Q3 = df_clean[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = df_clean[(df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)]\n",
    "        outlier_count = len(outliers)\n",
    "        outlier_percent = (outlier_count / len(df_clean)) * 100\n",
    "        \n",
    "        outlier_summary.append({\n",
    "            'Column': col,\n",
    "            'Outlier_Count': outlier_count,\n",
    "            'Outlier_Percentage': round(outlier_percent, 2),\n",
    "            'Lower_Bound': round(lower_bound, 2),\n",
    "            'Upper_Bound': round(upper_bound, 2),\n",
    "            'Action': 'Review' if outlier_percent > 5 else 'Normal'\n",
    "        })\n",
    "\n",
    "if outlier_summary:\n",
    "    outlier_df = pd.DataFrame(outlier_summary)\n",
    "    display(outlier_df)\n",
    "    \n",
    "    # Flag columns with excessive outliers\n",
    "    high_outlier_cols = outlier_df[outlier_df['Outlier_Percentage'] > 10]['Column'].tolist()\n",
    "    if high_outlier_cols:\n",
    "        print(f\"‚ö†Ô∏è  Columns with high outlier rates (>10%): {high_outlier_cols}\")\n",
    "        print(\"   ‚Üí Consider business rules for handling these extreme values\")\n",
    "\n",
    "# Summary of cleaning operations\n",
    "final_rows = len(df_clean)\n",
    "rows_removed = initial_rows - final_rows\n",
    "missing_after = df_clean.isnull().sum().sum()\n",
    "memory_after = df_clean.memory_usage(deep=True).sum()\n",
    "memory_saved = memory_before - memory_after\n",
    "\n",
    "print(f\"\\n‚úÖ INTELLIGENT CLEANING SUMMARY:\")\n",
    "print(\"-\" * 35)\n",
    "print(f\"‚Ä¢ Initial rows: {initial_rows:,}\")\n",
    "print(f\"‚Ä¢ Final rows: {final_rows:,}\")\n",
    "print(f\"‚Ä¢ Rows removed: {rows_removed:,} ({(rows_removed/initial_rows*100):.1f}%)\")\n",
    "print(f\"‚Ä¢ Missing values before: {missing_before:,}\")\n",
    "print(f\"‚Ä¢ Missing values after: {missing_after:,}\")\n",
    "print(f\"‚Ä¢ Data completeness: {((len(df_clean) * len(df_clean.columns) - missing_after) / (len(df_clean) * len(df_clean.columns)) * 100):.1f}%\")\n",
    "print(f\"‚Ä¢ Memory before: {memory_before / 1024**2:.2f} MB\")\n",
    "print(f\"‚Ä¢ Memory after: {memory_after / 1024**2:.2f} MB\")\n",
    "print(f\"‚Ä¢ Memory optimization: {memory_saved / 1024**2:.2f} MB saved ({(memory_saved/memory_before*100):.1f}%)\")\n",
    "\n",
    "print(f\"\\nüéØ CLEANED DATASET READY FOR BUSINESS ANALYSIS:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"‚Ä¢ Numerical measures: {len(numerical_cols_clean)} columns\")\n",
    "print(f\"‚Ä¢ Categorical features: {len(categorical_cols_clean)} columns\")\n",
    "print(f\"‚Ä¢ Date fields: {len(date_cols_clean)} columns\")\n",
    "print(f\"‚Ä¢ Derived features: {len(df_clean.columns) - len(df_for_analysis.columns)} new columns\")\n",
    "print(f\"‚Ä¢ ID columns excluded: ‚úÖ (focusing on business value)\")\n",
    "print(f\"‚Ä¢ Ready for meaningful analysis: ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50a87b3",
   "metadata": {},
   "source": [
    "## 5. Statistical Analysis\n",
    "\n",
    "Performing descriptive statistics, correlation analysis, and identifying key patterns and trends in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa47e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intelligent Statistical Analysis (Data Type Aware)\n",
    "print(\"üìä INTELLIGENT STATISTICAL ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 1. NUMERICAL MEASURES ANALYSIS\n",
    "if len(numerical_cols) > 0:\n",
    "    print(\"üî¢ NUMERICAL MEASURES DEEP DIVE\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    numerical_data = df_for_analysis[numerical_cols]\n",
    "    \n",
    "    # Enhanced descriptive statistics\n",
    "    desc_stats = numerical_data.describe()\n",
    "    \n",
    "    # Add meaningful additional statistics\n",
    "    additional_stats = pd.DataFrame({\n",
    "        'skewness': numerical_data.skew(),\n",
    "        'kurtosis': numerical_data.kurtosis(),\n",
    "        'coefficient_of_variation': (numerical_data.std() / numerical_data.mean()) * 100,\n",
    "        'outlier_count': [(abs(numerical_data[col] - numerical_data[col].mean()) > 2*numerical_data[col].std()).sum() \n",
    "                         for col in numerical_cols],\n",
    "        'outlier_percentage': [(abs(numerical_data[col] - numerical_data[col].mean()) > 2*numerical_data[col].std()).sum() / len(numerical_data) * 100\n",
    "                             for col in numerical_cols]\n",
    "    }, index=numerical_cols).round(3)\n",
    "    \n",
    "    print(\"üìà Extended Descriptive Statistics:\")\n",
    "    display(desc_stats.round(2))\n",
    "    \n",
    "    print(\"\\nüìê Distribution Characteristics:\")\n",
    "    display(additional_stats)\n",
    "    \n",
    "    # Business insights from numerical data\n",
    "    print(f\"\\nüí° BUSINESS INSIGHTS FROM NUMERICAL DATA:\")\n",
    "    print(\"-\" * 45)\n",
    "    for col in numerical_cols:\n",
    "        values = numerical_data[col]\n",
    "        cv = (values.std() / values.mean()) * 100\n",
    "        skewness = values.skew()\n",
    "        \n",
    "        print(f\"\\nüìä {col.upper()}:\")\n",
    "        if cv < 15:\n",
    "            print(f\"   ‚Ä¢ Low variability (CV: {cv:.1f}%) - Consistent values\")\n",
    "        elif cv > 50:\n",
    "            print(f\"   ‚Ä¢ High variability (CV: {cv:.1f}%) - Wide range of values\")\n",
    "        else:\n",
    "            print(f\"   ‚Ä¢ Moderate variability (CV: {cv:.1f}%) - Normal business range\")\n",
    "            \n",
    "        if abs(skewness) < 0.5:\n",
    "            print(f\"   ‚Ä¢ Normal distribution - Balanced data\")\n",
    "        elif skewness > 1:\n",
    "            print(f\"   ‚Ä¢ Right-skewed - Most values are lower, some high outliers\")\n",
    "        elif skewness < -1:\n",
    "            print(f\"   ‚Ä¢ Left-skewed - Most values are higher, some low outliers\")\n",
    "    \n",
    "    # Meaningful correlation analysis (only between numerical measures)\n",
    "    if len(numerical_cols) > 1:\n",
    "        print(f\"\\nüîó NUMERICAL CORRELATIONS ANALYSIS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        correlation_matrix = numerical_data.corr()\n",
    "        \n",
    "        # Find meaningful correlations\n",
    "        strong_correlations = []\n",
    "        moderate_correlations = []\n",
    "        \n",
    "        for i in range(len(correlation_matrix.columns)):\n",
    "            for j in range(i+1, len(correlation_matrix.columns)):\n",
    "                corr_value = correlation_matrix.iloc[i, j]\n",
    "                var1 = correlation_matrix.columns[i]\n",
    "                var2 = correlation_matrix.columns[j]\n",
    "                \n",
    "                if abs(corr_value) > 0.7:\n",
    "                    strong_correlations.append({\n",
    "                        'Variable 1': var1,\n",
    "                        'Variable 2': var2,\n",
    "                        'Correlation': round(corr_value, 3),\n",
    "                        'Interpretation': 'Strong Positive' if corr_value > 0.7 else 'Strong Negative',\n",
    "                        'Business_Meaning': f\"{'Higher' if corr_value > 0 else 'Lower'} {var1} typically means {'higher' if corr_value > 0 else 'lower'} {var2}\"\n",
    "                    })\n",
    "                elif abs(corr_value) > 0.4:\n",
    "                    moderate_correlations.append({\n",
    "                        'Variable 1': var1,\n",
    "                        'Variable 2': var2,\n",
    "                        'Correlation': round(corr_value, 3),\n",
    "                        'Interpretation': 'Moderate Positive' if corr_value > 0.4 else 'Moderate Negative'\n",
    "                    })\n",
    "        \n",
    "        if strong_correlations:\n",
    "            print(\"üí™ Strong Correlations (|r| > 0.7) - Key Business Relationships:\")\n",
    "            strong_corr_df = pd.DataFrame(strong_correlations)\n",
    "            display(strong_corr_df)\n",
    "        \n",
    "        if moderate_correlations:\n",
    "            print(f\"\\nüìà Moderate Correlations (0.4 < |r| < 0.7) - Notable Relationships:\")\n",
    "            moderate_corr_df = pd.DataFrame(moderate_correlations)\n",
    "            display(moderate_corr_df[['Variable 1', 'Variable 2', 'Correlation', 'Interpretation']])\n",
    "        \n",
    "        if not strong_correlations and not moderate_correlations:\n",
    "            print(\"‚ÑπÔ∏è  No significant correlations found - Variables appear independent\")\n",
    "        \n",
    "        print(f\"\\nüìã Full Numerical Correlation Matrix:\")\n",
    "        display(correlation_matrix.round(3))\n",
    "\n",
    "# 2. CATEGORICAL FEATURES ANALYSIS\n",
    "if len(categorical_cols) > 0:\n",
    "    print(f\"\\nüè∑Ô∏è  CATEGORICAL FEATURES BUSINESS ANALYSIS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        print(f\"\\nüìä {col.upper()} BUSINESS DISTRIBUTION:\")\n",
    "        value_counts = df_for_analysis[col].value_counts()\n",
    "        value_percentages = (value_counts / len(df_for_analysis) * 100).round(1)\n",
    "        \n",
    "        # Create comprehensive categorical summary\n",
    "        cat_summary = pd.DataFrame({\n",
    "            'Count': value_counts,\n",
    "            'Percentage': value_percentages,\n",
    "            'Business_Impact': ['High' if pct > 50 else 'Medium' if pct > 20 else 'Low' \n",
    "                              for pct in value_percentages]\n",
    "        })\n",
    "        \n",
    "        display(cat_summary)\n",
    "        \n",
    "        # Business insights\n",
    "        dominant_category = value_counts.index[0]\n",
    "        dominant_pct = value_percentages.iloc[0]\n",
    "        print(f\"   üí° Dominant category: {dominant_category} ({dominant_pct}%)\")\n",
    "        \n",
    "        if dominant_pct > 70:\n",
    "            print(f\"   ‚ö†Ô∏è  Highly concentrated distribution - {dominant_category} dominates\")\n",
    "        elif len(value_counts) > 10:\n",
    "            print(f\"   üìä High diversity - {len(value_counts)} different categories\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ Balanced distribution across {len(value_counts)} categories\")\n",
    "\n",
    "# 3. TEMPORAL ANALYSIS (Business Time Patterns)\n",
    "if len(date_cols) > 0:\n",
    "    print(f\"\\nüìÖ TEMPORAL BUSINESS PATTERNS ANALYSIS\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    for col in date_cols:\n",
    "        print(f\"\\nüïí {col.upper()} TEMPORAL INSIGHTS:\")\n",
    "        date_series = df_for_analysis[col]\n",
    "        \n",
    "        # Business-relevant temporal patterns\n",
    "        print(f\"   üìÖ Business Timeline: {date_series.min().strftime('%Y-%m-%d')} to {date_series.max().strftime('%Y-%m-%d')}\")\n",
    "        print(f\"   ‚è±Ô∏è  Data Coverage: {(date_series.max() - date_series.min()).days:,} days\")\n",
    "        \n",
    "        # Year-over-year analysis\n",
    "        if hasattr(date_series.dt, 'year'):\n",
    "            year_counts = date_series.dt.year.value_counts().sort_index()\n",
    "            print(f\"   üìä Records by Year: {dict(year_counts)}\")\n",
    "            \n",
    "            if len(year_counts) > 1:\n",
    "                growth_rate = ((year_counts.iloc[-1] - year_counts.iloc[0]) / year_counts.iloc[0] * 100)\n",
    "                print(f\"   üìà Growth Rate: {growth_rate:.1f}% from first to last year\")\n",
    "        \n",
    "        # Seasonal patterns (meaningful for business)\n",
    "        if hasattr(date_series.dt, 'month'):\n",
    "            month_counts = date_series.dt.month.value_counts().sort_index()\n",
    "            peak_month = month_counts.idxmax()\n",
    "            low_month = month_counts.idxmin()\n",
    "            month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "            print(f\"   üîù Peak Activity: {month_names[peak_month-1]} ({month_counts.max()} records)\")\n",
    "            print(f\"   üìâ Lowest Activity: {month_names[low_month-1]} ({month_counts.min()} records)\")\n",
    "\n",
    "# 4. CROSS-TYPE ANALYSIS (Categorical vs Numerical)\n",
    "if len(categorical_cols) > 0 and len(numerical_cols) > 0:\n",
    "    print(f\"\\n\udd04 CROSS-TYPE BUSINESS ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for cat_col in categorical_cols:\n",
    "        for num_col in numerical_cols:\n",
    "            print(f\"\\n\udcca {num_col.upper()} by {cat_col.upper()}:\")\n",
    "            \n",
    "            grouped_stats = df_for_analysis.groupby(cat_col)[num_col].agg([\n",
    "                'count', 'mean', 'median', 'std'\n",
    "            ]).round(2)\n",
    "            \n",
    "            # Add business interpretation\n",
    "            grouped_stats['cv'] = (grouped_stats['std'] / grouped_stats['mean'] * 100).round(1)\n",
    "            grouped_stats['relative_performance'] = (\n",
    "                (grouped_stats['mean'] - grouped_stats['mean'].mean()) / \n",
    "                grouped_stats['mean'].mean() * 100\n",
    "            ).round(1)\n",
    "            \n",
    "            display(grouped_stats)\n",
    "            \n",
    "            # Business insights\n",
    "            best_category = grouped_stats['mean'].idxmax()\n",
    "            worst_category = grouped_stats['mean'].idxmin()\n",
    "            performance_gap = ((grouped_stats['mean'].max() - grouped_stats['mean'].min()) / \n",
    "                             grouped_stats['mean'].mean() * 100)\n",
    "            \n",
    "            print(f\"   üèÜ Best Performing: {best_category} (avg: {grouped_stats.loc[best_category, 'mean']:.2f})\")\n",
    "            print(f\"   üìâ Lowest Performing: {worst_category} (avg: {grouped_stats.loc[worst_category, 'mean']:.2f})\")\n",
    "            print(f\"   üìä Performance Gap: {performance_gap:.1f}%\")\n",
    "\n",
    "print(f\"\\n‚úÖ INTELLIGENT ANALYSIS COMPLETE\")\n",
    "print(\"-\" * 35)\n",
    "print(\"üéØ Analysis focused on meaningful business relationships\")\n",
    "print(\"üö´ Avoided inappropriate operations on ID and date columns\")\n",
    "print(\"üìä Provided actionable business insights from data patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b99d241",
   "metadata": {},
   "source": [
    "## 6. Data Visualization\n",
    "\n",
    "Creating comprehensive visualizations to understand patterns, distributions, and relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cd6c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visualization\n",
    "print(\"üìä CREATING COMPREHENSIVE VISUALIZATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Get numerical and categorical columns\n",
    "numerical_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df_clean.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# 1. Distribution Analysis for Numerical Variables\n",
    "if len(numerical_cols) > 0:\n",
    "    print(\"üìà Creating distribution plots for numerical variables...\")\n",
    "    \n",
    "    # Calculate number of rows needed\n",
    "    n_cols = min(3, len(numerical_cols))\n",
    "    n_rows = (len(numerical_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = [axes] if n_cols == 1 else axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        if i < len(axes):\n",
    "            # Create histogram with KDE\n",
    "            sns.histplot(data=df_clean, x=col, kde=True, ax=axes[i])\n",
    "            axes[i].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add statistics text\n",
    "            mean_val = df_clean[col].mean()\n",
    "            median_val = df_clean[col].median()\n",
    "            std_val = df_clean[col].std()\n",
    "            axes[i].axvline(mean_val, color='red', linestyle='--', alpha=0.7, label=f'Mean: {mean_val:.2f}')\n",
    "            axes[i].axvline(median_val, color='green', linestyle='--', alpha=0.7, label=f'Median: {median_val:.2f}')\n",
    "            axes[i].legend()\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(numerical_cols), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 2. Categorical Variables Visualization\n",
    "if len(categorical_cols) > 0:\n",
    "    print(\"üè∑Ô∏è  Creating categorical variable plots...\")\n",
    "    \n",
    "    for col in categorical_cols[:4]:  # Limit to first 4 categorical columns\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Create subplots for count plot and pie chart\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Count plot\n",
    "        value_counts = df_clean[col].value_counts()\n",
    "        top_categories = value_counts.head(10)  # Show top 10 categories\n",
    "        \n",
    "        sns.countplot(data=df_clean[df_clean[col].isin(top_categories.index)], \n",
    "                     x=col, order=top_categories.index, ax=ax1)\n",
    "        ax1.set_title(f'Count Distribution of {col}', fontsize=14, fontweight='bold')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Pie chart for top categories\n",
    "        if len(top_categories) <= 8:  # Only create pie chart if not too many categories\n",
    "            colors = plt.cm.Set3(np.linspace(0, 1, len(top_categories)))\n",
    "            wedges, texts, autotexts = ax2.pie(top_categories.values, \n",
    "                                              labels=top_categories.index,\n",
    "                                              autopct='%1.1f%%',\n",
    "                                              colors=colors,\n",
    "                                              startangle=90)\n",
    "            ax2.set_title(f'{col} Distribution', fontsize=14, fontweight='bold')\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, f'Too many categories\\\\nto display in pie chart\\\\n({len(value_counts)} unique values)',\n",
    "                    ha='center', va='center', transform=ax2.transAxes, fontsize=12)\n",
    "            ax2.set_xlim(0, 1)\n",
    "            ax2.set_ylim(0, 1)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# 3. Correlation Heatmap\n",
    "if len(numerical_cols) > 1:\n",
    "    print(\"üîó Creating correlation heatmap...\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    correlation_matrix = df_clean[numerical_cols].corr()\n",
    "    \n",
    "    # Create mask for upper triangle\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(correlation_matrix, \n",
    "                mask=mask,\n",
    "                annot=True, \n",
    "                cmap='RdBu_r',\n",
    "                center=0,\n",
    "                square=True,\n",
    "                fmt='.2f',\n",
    "                cbar_kws={\"shrink\": .8})\n",
    "    \n",
    "    plt.title('Correlation Matrix of Numerical Variables', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 4. Box Plots for Outlier Visualization\n",
    "if len(numerical_cols) > 0:\n",
    "    print(\"üì¶ Creating box plots for outlier detection...\")\n",
    "    \n",
    "    n_cols = min(3, len(numerical_cols))\n",
    "    n_rows = (len(numerical_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = [axes] if n_cols == 1 else axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        if i < len(axes):\n",
    "            sns.boxplot(data=df_clean, y=col, ax=axes[i])\n",
    "            axes[i].set_title(f'Box Plot of {col}', fontsize=12, fontweight='bold')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(numerical_cols), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 5. Relationship Analysis (if multiple numerical variables)\n",
    "if len(numerical_cols) >= 2:\n",
    "    print(\"üîç Creating relationship analysis plots...\")\n",
    "    \n",
    "    # Scatter plot matrix for first 4 numerical variables\n",
    "    cols_to_plot = numerical_cols[:4]\n",
    "    if len(cols_to_plot) >= 2:\n",
    "        fig = sns.pairplot(df_clean[cols_to_plot], diag_kind='hist', height=2.5)\n",
    "        fig.suptitle('Pairwise Relationships Between Numerical Variables', \n",
    "                    y=1.02, fontsize=16, fontweight='bold')\n",
    "        plt.show()\n",
    "\n",
    "# 6. Time Series Analysis (if datetime columns exist)\n",
    "datetime_cols = df_clean.select_dtypes(include=['datetime64']).columns\n",
    "if len(datetime_cols) > 0 and len(numerical_cols) > 0:\n",
    "    print(\"üìÖ Creating time series analysis...\")\n",
    "    \n",
    "    for date_col in datetime_cols[:2]:  # Limit to first 2 datetime columns\n",
    "        for num_col in numerical_cols[:2]:  # Limit to first 2 numerical columns\n",
    "            plt.figure(figsize=(14, 6))\n",
    "            \n",
    "            # Create monthly aggregation\n",
    "            df_monthly = df_clean.groupby(df_clean[date_col].dt.to_period('M'))[num_col].agg(['mean', 'count']).reset_index()\n",
    "            df_monthly[date_col] = df_monthly[date_col].dt.to_timestamp()\n",
    "            \n",
    "            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "            \n",
    "            # Time series plot\n",
    "            ax1.plot(df_monthly[date_col], df_monthly['mean'], marker='o', linewidth=2, markersize=6)\n",
    "            ax1.set_title(f'Monthly Average {num_col} Over Time', fontsize=14, fontweight='bold')\n",
    "            ax1.set_ylabel(f'Average {num_col}')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            ax1.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Count plot\n",
    "            ax2.bar(df_monthly[date_col], df_monthly['count'], alpha=0.7, color='skyblue')\n",
    "            ax2.set_title(f'Monthly Record Count Over Time', fontsize=14, fontweight='bold')\n",
    "            ax2.set_ylabel('Number of Records')\n",
    "            ax2.set_xlabel('Date')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            ax2.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "print(\"‚úÖ All visualizations completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc31188",
   "metadata": {},
   "source": [
    "## 7. Export Results and Summary\n",
    "\n",
    "Saving processed data and generating comprehensive analysis summary for stakeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a48fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Results and Generate Summary Report\n",
    "print(\"üíæ EXPORTING RESULTS AND GENERATING SUMMARY\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create exports directory if it doesn't exist\n",
    "export_dir = \"../exports\"\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "# Generate timestamp for file naming\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# 1. Export cleaned dataset\n",
    "print(\"üìÅ Exporting cleaned dataset...\")\n",
    "cleaned_file = f\"{export_dir}/dimcyallaccounts_cleaned_{timestamp}.csv\"\n",
    "df_clean.to_csv(cleaned_file, index=False)\n",
    "print(f\"   ‚úÖ Cleaned data exported to: {cleaned_file}\")\n",
    "\n",
    "# 2. Export summary statistics\n",
    "print(\"üìä Exporting summary statistics...\")\n",
    "if len(df_clean.select_dtypes(include=[np.number]).columns) > 0:\n",
    "    stats_file = f\"{export_dir}/summary_statistics_{timestamp}.csv\"\n",
    "    summary_stats = df_clean.describe(include='all')\n",
    "    summary_stats.to_csv(stats_file)\n",
    "    print(f\"   ‚úÖ Summary statistics exported to: {stats_file}\")\n",
    "\n",
    "# 3. Export correlation matrix\n",
    "numerical_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "if len(numerical_cols) > 1:\n",
    "    print(\"üîó Exporting correlation matrix...\")\n",
    "    corr_file = f\"{export_dir}/correlation_matrix_{timestamp}.csv\"\n",
    "    correlation_matrix = df_clean[numerical_cols].corr()\n",
    "    correlation_matrix.to_csv(corr_file)\n",
    "    print(f\"   ‚úÖ Correlation matrix exported to: {corr_file}\")\n",
    "\n",
    "# 4. Generate Executive Summary Report\n",
    "print(\"üìã Generating executive summary report...\")\n",
    "report_file = f\"{export_dir}/executive_summary_{timestamp}.txt\"\n",
    "\n",
    "with open(report_file, 'w') as f:\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(\"ONELAKE DIMCYALLACCOUNTS TABLE - EXECUTIVE SUMMARY REPORT\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(f\"Analysis Date: {datetime.now().strftime('%B %d, %Y at %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Data Source: Microsoft Fabric OneLake - CPE Staging Lake\\n\")\n",
    "    f.write(f\"Table: dimcyallaccounts\\n\")\n",
    "    f.write(f\"Analysis Framework: Python with Azure SDK Integration\\n\\n\")\n",
    "    \n",
    "    # Dataset Overview\n",
    "    f.write(\"DATASET OVERVIEW\\n\")\n",
    "    f.write(\"-\"*50 + \"\\n\")\n",
    "    f.write(f\"‚Ä¢ Total Records: {len(df_clean):,}\\n\")\n",
    "    f.write(f\"‚Ä¢ Total Variables: {len(df_clean.columns)}\\n\")\n",
    "    f.write(f\"‚Ä¢ Numerical Variables: {len(numerical_cols)}\\n\")\n",
    "    f.write(f\"‚Ä¢ Categorical Variables: {len(df_clean.select_dtypes(include=['object', 'category']).columns)}\\n\")\n",
    "    f.write(f\"‚Ä¢ Data Completeness: {((len(df_clean) * len(df_clean.columns) - df_clean.isnull().sum().sum()) / (len(df_clean) * len(df_clean.columns)) * 100):.1f}%\\n\")\n",
    "    f.write(f\"‚Ä¢ Memory Usage: {df_clean.memory_usage(deep=True).sum() / 1024**2:.2f} MB\\n\\n\")\n",
    "    \n",
    "    # Data Quality Assessment\n",
    "    f.write(\"DATA QUALITY ASSESSMENT\\n\")\n",
    "    f.write(\"-\"*35 + \"\\n\")\n",
    "    missing_data = df_clean.isnull().sum()\n",
    "    total_missing = missing_data.sum()\n",
    "    f.write(f\"‚Ä¢ Total Missing Values: {total_missing:,}\\n\")\n",
    "    f.write(f\"‚Ä¢ Missing Data Rate: {(total_missing / (len(df_clean) * len(df_clean.columns)) * 100):.2f}%\\n\")\n",
    "    f.write(f\"‚Ä¢ Duplicate Records: {df_clean.duplicated().sum():,}\\n\")\n",
    "    f.write(f\"‚Ä¢ Data Integrity: {'Excellent' if total_missing < len(df_clean) * 0.05 else 'Good' if total_missing < len(df_clean) * 0.1 else 'Needs Attention'}\\n\\n\")\n",
    "    \n",
    "    # Key Statistics\n",
    "    if len(numerical_cols) > 0:\n",
    "        f.write(\"KEY NUMERICAL STATISTICS\\n\")\n",
    "        f.write(\"-\"*35 + \"\\n\")\n",
    "        for col in numerical_cols[:5]:  # Top 5 numerical columns\n",
    "            f.write(f\"‚Ä¢ {col}:\\n\")\n",
    "            f.write(f\"  - Mean: {df_clean[col].mean():.2f}\\n\")\n",
    "            f.write(f\"  - Median: {df_clean[col].median():.2f}\\n\")\n",
    "            f.write(f\"  - Std Dev: {df_clean[col].std():.2f}\\n\")\n",
    "            f.write(f\"  - Range: [{df_clean[col].min():.2f}, {df_clean[col].max():.2f}]\\n\")\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "    # Categorical Analysis\n",
    "    categorical_cols = df_clean.select_dtypes(include=['object', 'category']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        f.write(\"CATEGORICAL VARIABLES SUMMARY\\n\")\n",
    "        f.write(\"-\"*40 + \"\\n\")\n",
    "        for col in categorical_cols[:3]:  # Top 3 categorical columns\n",
    "            f.write(f\"‚Ä¢ {col}:\\n\")\n",
    "            f.write(f\"  - Unique Values: {df_clean[col].nunique()}\\n\")\n",
    "            f.write(f\"  - Most Frequent: '{df_clean[col].value_counts().index[0]}' ({df_clean[col].value_counts(normalize=True).iloc[0]*100:.1f}%)\\n\")\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "    # Recommendations\n",
    "    f.write(\"KEY INSIGHTS AND RECOMMENDATIONS\\n\")\n",
    "    f.write(\"-\"*45 + \"\\n\")\n",
    "    \n",
    "    # Data quality recommendations\n",
    "    if total_missing > 0:\n",
    "        f.write(\"‚Ä¢ Data Quality: Consider implementing data validation rules to reduce missing values\\n\")\n",
    "    else:\n",
    "        f.write(\"‚Ä¢ Data Quality: Excellent data completeness - maintain current data governance standards\\n\")\n",
    "    \n",
    "    # Performance recommendations\n",
    "    memory_mb = df_clean.memory_usage(deep=True).sum() / 1024**2\n",
    "    if memory_mb > 100:\n",
    "        f.write(\"‚Ä¢ Performance: Consider data type optimization and partitioning for large datasets\\n\")\n",
    "    else:\n",
    "        f.write(\"‚Ä¢ Performance: Current dataset size is manageable for in-memory analysis\\n\")\n",
    "    \n",
    "    # Business insights\n",
    "    if len(numerical_cols) > 0:\n",
    "        high_variation_cols = [col for col in numerical_cols if (df_clean[col].std() / df_clean[col].mean()) > 1]\n",
    "        if high_variation_cols:\n",
    "            f.write(f\"‚Ä¢ Business Insights: High variation detected in {', '.join(high_variation_cols[:2])} - investigate underlying patterns\\n\")\n",
    "    \n",
    "    # Next steps\n",
    "    f.write(\"\\nRECOMMENDED NEXT STEPS\\n\")\n",
    "    f.write(\"-\"*30 + \"\\n\")\n",
    "    f.write(\"1. Implement automated data quality monitoring\\n\")\n",
    "    f.write(\"2. Establish baseline metrics for ongoing comparison\\n\")\n",
    "    f.write(\"3. Create scheduled analysis pipeline for regular insights\\n\")\n",
    "    f.write(\"4. Integrate findings with business intelligence dashboards\\n\")\n",
    "    f.write(\"5. Consider predictive modeling for key business metrics\\n\\n\")\n",
    "    \n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(\"Report generated by Alex Cognitive Architecture - Fishbowl POC\\n\")\n",
    "    f.write(\"Azure Enterprise Data Platform Analysis Framework\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(f\"   ‚úÖ Executive summary exported to: {report_file}\")\n",
    "\n",
    "# 5. Create data dictionary\n",
    "print(\"üìö Creating data dictionary...\")\n",
    "dict_file = f\"{export_dir}/data_dictionary_{timestamp}.csv\"\n",
    "\n",
    "data_dict = []\n",
    "for col in df_clean.columns:\n",
    "    data_dict.append({\n",
    "        'Column_Name': col,\n",
    "        'Data_Type': str(df_clean[col].dtype),\n",
    "        'Non_Null_Count': df_clean[col].count(),\n",
    "        'Null_Count': df_clean[col].isnull().sum(),\n",
    "        'Unique_Values': df_clean[col].nunique(),\n",
    "        'Sample_Values': str(df_clean[col].dropna().head(3).tolist())\n",
    "    })\n",
    "\n",
    "data_dict_df = pd.DataFrame(data_dict)\n",
    "data_dict_df.to_csv(dict_file, index=False)\n",
    "print(f\"   ‚úÖ Data dictionary exported to: {dict_file}\")\n",
    "\n",
    "# 6. Generate final summary\n",
    "print(f\"\\nüéâ ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìä Dataset Size: {len(df_clean):,} records √ó {len(df_clean.columns)} columns\")\n",
    "print(f\"üîç Analysis Quality: {((len(df_clean) * len(df_clean.columns) - df_clean.isnull().sum().sum()) / (len(df_clean) * len(df_clean.columns)) * 100):.1f}% data completeness\")\n",
    "print(f\"üíæ Files Exported: {len([f for f in os.listdir(export_dir) if timestamp in f])} files\")\n",
    "print(f\"üìÅ Export Location: {os.path.abspath(export_dir)}\")\n",
    "\n",
    "print(f\"\\nüìã Exported Files:\")\n",
    "print(\"-\" * 20)\n",
    "for file in os.listdir(export_dir):\n",
    "    if timestamp in file:\n",
    "        print(f\"   ‚Ä¢ {file}\")\n",
    "\n",
    "print(f\"\\nüí° Key Findings:\")\n",
    "print(\"-\" * 15)\n",
    "if len(numerical_cols) > 0:\n",
    "    print(f\"   ‚Ä¢ {len(numerical_cols)} numerical variables analyzed\")\n",
    "    print(f\"   ‚Ä¢ Correlation analysis completed for relationship mapping\")\n",
    "if len(categorical_cols) > 0:\n",
    "    print(f\"   ‚Ä¢ {len(categorical_cols)} categorical variables profiled\")\n",
    "print(f\"   ‚Ä¢ Data quality assessment: {'Excellent' if df_clean.isnull().sum().sum() < len(df_clean) * 0.05 else 'Good'}\")\n",
    "print(f\"   ‚Ä¢ Ready for advanced analytics and machine learning\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps: Review executive summary and implement recommended data governance practices\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
