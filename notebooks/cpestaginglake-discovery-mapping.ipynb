{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ca325e4",
   "metadata": {},
   "source": [
    "# Storage Account Discovery & SQL Endpoint Mapping\n",
    "\n",
    "**Purpose**: Automatically discover all data folders in the configured storage account and create shortcuts/tables for SQL endpoint access.\n",
    "\n",
    "**Storage Account**: *Configured in setup cell*  \n",
    "**Data Format**: Parquet files  \n",
    "**Output**: Shortcuts and external tables for Fabric SQL endpoint  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9375d5db",
   "metadata": {},
   "source": [
    "## üìã Configuration & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdb13dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ CPE Staging Lake Discovery Configuration Loaded\n",
      "üì¶ Storage Account: cpestaginglake\n",
      "üìÑ Expected Format: PARQUET\n",
      "üîç Max Search Depth: 3\n",
      "üìä Table Prefix: staging_\n"
     ]
    }
   ],
   "source": [
    "# Storage Account Discovery Configuration\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Load environment variables from .env file\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv('../.env')  # Load from parent directory\n",
    "    print(\"‚úÖ Environment variables loaded from .env file\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è python-dotenv not installed. Using default configuration.\")\n",
    "    print(\"üí° Run: pip install python-dotenv to use .env file\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load .env file: {str(e)}\")\n",
    "\n",
    "# Storage account configuration (with .env fallback)\n",
    "STORAGE_ACCOUNT = os.getenv('STORAGE_ACCOUNT_NAME', 'cpestaginglake')\n",
    "BASE_URL = os.getenv('STORAGE_BASE_URL', f\"abfss://{{container}}@{STORAGE_ACCOUNT}.dfs.core.windows.net/\")\n",
    "DATA_FORMAT = os.getenv('DATA_FORMAT', 'PARQUET')\n",
    "\n",
    "# Workspace configuration for authentication compliance\n",
    "WORKSPACE_ID = os.getenv('FABRIC_WORKSPACE_ID', None)\n",
    "\n",
    "# Try to get workspace ID automatically if not in env\n",
    "if not WORKSPACE_ID:\n",
    "    try:\n",
    "        from notebookutils import mssparkutils\n",
    "        WORKSPACE_ID = mssparkutils.env.getWorkspaceId()\n",
    "        print(f\"üîç Auto-detected Workspace ID: {WORKSPACE_ID}\")\n",
    "    except Exception as workspace_error:\n",
    "        print(f\"‚ö†Ô∏è Could not auto-detect workspace ID: {str(workspace_error)}\")\n",
    "        print(\"üí° You may need to set FABRIC_WORKSPACE_ID in .env file\")\n",
    "else:\n",
    "    print(f\"üîç Using Workspace ID from .env: {WORKSPACE_ID}\")\n",
    "\n",
    "# Discovery settings (with .env fallback)\n",
    "DISCOVERY_CONFIG = {\n",
    "    \"max_depth\": int(os.getenv('MAX_SEARCH_DEPTH', '3')),\n",
    "    \"min_files\": int(os.getenv('MIN_FILES_PER_FOLDER', '1')),\n",
    "    \"file_extensions\": os.getenv('FILE_EXTENSIONS', '.parquet,.pqt').split(','),\n",
    "    \"exclude_folders\": os.getenv('EXCLUDE_FOLDERS', '_tmp,_temp,_logs,.spark').split(','),\n",
    "    \"table_prefix\": os.getenv('TABLE_PREFIX', 'staging_'),\n",
    "    \"use_workspace_id\": os.getenv('USE_WORKSPACE_ID', 'true').lower() == 'true',\n",
    "}\n",
    "\n",
    "# Results tracking\n",
    "discovery_results = {\n",
    "    \"containers\": [],\n",
    "    \"data_folders\": [],\n",
    "    \"shortcuts_created\": [],\n",
    "    \"tables_created\": [],\n",
    "    \"errors\": []\n",
    "}\n",
    "\n",
    "# Environment information\n",
    "FABRIC_WORKSPACE_NAME = os.getenv('FABRIC_WORKSPACE_NAME', 'Unknown')\n",
    "FABRIC_ENVIRONMENT = os.getenv('FABRIC_ENVIRONMENT', 'Unknown')\n",
    "PROJECT_NAME = os.getenv('PROJECT_NAME', 'Storage Discovery')\n",
    "\n",
    "print(f\"üöÄ {PROJECT_NAME.upper()} Discovery Configuration Loaded\")\n",
    "print(f\"üì¶ Storage Account: {STORAGE_ACCOUNT}\")\n",
    "print(f\"üè¢ Workspace: {FABRIC_WORKSPACE_NAME} ({WORKSPACE_ID or 'Not available'})\")\n",
    "print(f\"üåê Environment: {FABRIC_ENVIRONMENT}\")\n",
    "print(f\"üìÑ Expected Format: {DATA_FORMAT}\")\n",
    "print(f\"üîç Max Search Depth: {DISCOVERY_CONFIG['max_depth']}\")\n",
    "print(f\"üìä Table Prefix: {DISCOVERY_CONFIG['table_prefix']}\")\n",
    "print(f\"üîó Base URL Pattern: {BASE_URL}\")\n",
    "print(f\"üìÅ File Extensions: {', '.join(DISCOVERY_CONFIG['file_extensions'])}\")\n",
    "print(f\"üîê Auth Compliance: {'Enabled' if DISCOVERY_CONFIG['use_workspace_id'] else 'Disabled'}\")\n",
    "\n",
    "# Display key environment variables for verification\n",
    "env_vars = {\n",
    "    'STORAGE_ACCOUNT_NAME': STORAGE_ACCOUNT,\n",
    "    'FABRIC_WORKSPACE_ID': WORKSPACE_ID,\n",
    "    'FABRIC_WORKSPACE_NAME': FABRIC_WORKSPACE_NAME,\n",
    "    'DATA_FORMAT': DATA_FORMAT,\n",
    "    'TABLE_PREFIX': DISCOVERY_CONFIG['table_prefix']\n",
    "}\n",
    "\n",
    "print(f\"\\nüìã Active Configuration:\")\n",
    "for key, value in env_vars.items():\n",
    "    status = \"‚úÖ\" if value else \"‚ùå\"\n",
    "    print(f\"   {status} {key}: {value or 'Not set'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f2e060",
   "metadata": {},
   "source": [
    "## üîç Step 1: Container Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c15dcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting container discovery for cpestaginglake...\n",
      "üîç Discovering all containers in cpestaginglake...\n",
      "‚ùå mssparkutils failed: No module named 'notebookutils'\n",
      "üí° This might be due to:\n",
      "   - Storage account access permissions\n",
      "   - Incorrect storage account name\n",
      "   - Network connectivity issues\n",
      "\n",
      "üîß Attempting alternative discovery method...\n",
      "üí° If you know specific container names, please update the list below:\n",
      "‚ö†Ô∏è No known containers to test. Please check storage account access.\n",
      "‚ùå No containers discovered\n",
      "üí° Possible solutions:\n",
      "   - Check if 'cpestaginglake' is the correct storage account name\n",
      "   - Verify you have read access to the storage account\n",
      "   - Ensure the lakehouse is properly connected to the storage account\n",
      "\n",
      "üîç Troubleshooting Tips:\n",
      "1. Verify storage account name: 'cpestaginglake'\n",
      "2. Check if you have the correct permissions\n",
      "3. Ensure the storage account exists and is accessible\n",
      "4. Try running this in a Fabric notebook environment\n"
     ]
    }
   ],
   "source": [
    "# Discover all containers in the storage account\n",
    "def discover_containers():\n",
    "    \"\"\"\n",
    "    Discover all containers/folders in the configured storage account\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"üîç Discovering all containers in {STORAGE_ACCOUNT}...\")\n",
    "        \n",
    "        # Try using mssparkutils to list everything\n",
    "        try:\n",
    "            from notebookutils import mssparkutils\n",
    "            \n",
    "            # List all top-level folders/containers at the storage account level\n",
    "            storage_root = f\"abfss://{STORAGE_ACCOUNT}.dfs.core.windows.net/\"\n",
    "            print(f\"üìÇ Scanning: {storage_root}\")\n",
    "            \n",
    "            items = mssparkutils.fs.ls(storage_root)\n",
    "            \n",
    "            container_list = []\n",
    "            for item in items:\n",
    "                if item.isDir:\n",
    "                    container_name = item.name.rstrip('/')\n",
    "                    container_list.append({\n",
    "                        \"name\": container_name,\n",
    "                        \"path\": item.path,\n",
    "                        \"size\": item.size if hasattr(item, 'size') else 'unknown'\n",
    "                    })\n",
    "                    print(f\"   üì¶ Found container: {container_name}\")\n",
    "                    \n",
    "            discovery_results[\"containers\"] = container_list\n",
    "            print(f\"‚úÖ Found {len(container_list)} containers total\")\n",
    "            \n",
    "        except Exception as mssparkutils_error:\n",
    "            print(f\"‚ùå mssparkutils failed: {str(mssparkutils_error)}\")\n",
    "            print(\"üí° This might be due to:\")\n",
    "            print(\"   - Storage account access permissions\")\n",
    "            print(\"   - Incorrect storage account name\")\n",
    "            print(\"   - Network connectivity issues\")\n",
    "            \n",
    "            # Alternative: Try to list known containers if the root listing fails\n",
    "            print(\"\\nüîß Attempting alternative discovery method...\")\n",
    "            print(\"üí° If you know specific container names, please update the list below:\")\n",
    "            \n",
    "            # You can manually add known container names here if needed\n",
    "            known_containers = [\n",
    "                # Add your known container names here, for example:\n",
    "                # \"data\",\n",
    "                # \"raw\", \n",
    "                # \"processed\",\n",
    "                # \"staging\"\n",
    "            ]\n",
    "            \n",
    "            if known_containers:\n",
    "                found_containers = []\n",
    "                for container_name in known_containers:\n",
    "                    try:\n",
    "                        test_path = f\"abfss://{container_name}@{STORAGE_ACCOUNT}.dfs.core.windows.net/\"\n",
    "                        test_files = mssparkutils.fs.ls(test_path)\n",
    "                        \n",
    "                        found_containers.append({\n",
    "                            \"name\": container_name,\n",
    "                            \"path\": test_path,\n",
    "                            \"size\": len(test_files)\n",
    "                        })\n",
    "                        print(f\"   ‚úÖ Verified container: {container_name}\")\n",
    "                        \n",
    "                    except Exception as test_error:\n",
    "                        print(f\"   ‚ùå Cannot access container: {container_name}\")\n",
    "                        continue\n",
    "                        \n",
    "                discovery_results[\"containers\"] = found_containers\n",
    "                print(f\"‚úÖ Found {len(found_containers)} accessible containers\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è No known containers to test. Please check storage account access.\")\n",
    "                discovery_results[\"containers\"] = []\n",
    "        \n",
    "        # Display results\n",
    "        if discovery_results[\"containers\"]:\n",
    "            print(f\"\\nüì¶ Discovered Containers ({len(discovery_results['containers'])}):\")\n",
    "            for i, container in enumerate(discovery_results[\"containers\"], 1):\n",
    "                size_info = container.get('size', 'unknown')\n",
    "                if isinstance(size_info, int):\n",
    "                    size_display = f\"{size_info} items\"\n",
    "                else:\n",
    "                    size_display = str(size_info)\n",
    "                print(f\"   {i}. {container['name']} ({size_display})\")\n",
    "        else:\n",
    "            print(\"‚ùå No containers discovered\")\n",
    "            print(\"üí° Possible solutions:\")\n",
    "            print(f\"   - Check if '{STORAGE_ACCOUNT}' is the correct storage account name\")\n",
    "            print(\"   - Verify you have read access to the storage account\")\n",
    "            print(\"   - Ensure the lakehouse is properly connected to the storage account\")\n",
    "            \n",
    "        return discovery_results[\"containers\"]\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Container discovery failed: {str(e)}\"\n",
    "        discovery_results[\"errors\"].append(error_msg)\n",
    "        print(f\"‚ùå {error_msg}\")\n",
    "        return []\n",
    "\n",
    "# Execute container discovery\n",
    "print(f\"üöÄ Starting container discovery for {STORAGE_ACCOUNT}...\")\n",
    "containers = discover_containers()\n",
    "\n",
    "# Additional diagnostic information\n",
    "if not containers:\n",
    "    print(\"\\nüîç Troubleshooting Tips:\")\n",
    "    print(f\"1. Verify storage account name: '{STORAGE_ACCOUNT}'\")\n",
    "    print(\"2. Check if you have the correct permissions\")\n",
    "    print(\"3. Ensure the storage account exists and is accessible\")\n",
    "    print(\"4. Try running this in a Fabric notebook environment\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Ready to proceed with {len(containers)} containers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f69c72",
   "metadata": {},
   "source": [
    "## üìÅ Step 2: Data Folder Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f839df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover data folders within each container\n",
    "def discover_data_folders(containers):\n",
    "    \"\"\"\n",
    "    Recursively discover folders containing Parquet files in each container\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"üìÅ Discovering data folders with {DATA_FORMAT} files...\")\n",
    "        from notebookutils import mssparkutils\n",
    "        \n",
    "        all_data_folders = []\n",
    "        \n",
    "        for container in containers:\n",
    "            print(f\"\\nüîç Exploring container: {container['name']}\")\n",
    "            container_path = f\"abfss://{container['name']}@{STORAGE_ACCOUNT}.dfs.core.windows.net/\"\n",
    "            \n",
    "            # Recursive folder exploration\n",
    "            def explore_folder(folder_path, current_depth=0, relative_path=\"\"):\n",
    "                if current_depth > DISCOVERY_CONFIG[\"max_depth\"]:\n",
    "                    return []\n",
    "                \n",
    "                folder_results = []\n",
    "                \n",
    "                try:\n",
    "                    items = mssparkutils.fs.ls(folder_path)\n",
    "                    \n",
    "                    # Count Parquet files in current folder\n",
    "                    parquet_files = []\n",
    "                    subfolders = []\n",
    "                    \n",
    "                    for item in items:\n",
    "                        if item.isFile:\n",
    "                            if any(item.name.lower().endswith(ext) for ext in DISCOVERY_CONFIG[\"file_extensions\"]):\n",
    "                                parquet_files.append(item.name)\n",
    "                        elif item.isDir:\n",
    "                            folder_name = item.name.rstrip('/')\n",
    "                            if not any(excl in folder_name.lower() for excl in DISCOVERY_CONFIG[\"exclude_folders\"]):\n",
    "                                subfolders.append((item.path, folder_name))\n",
    "                    \n",
    "                    # If this folder has enough Parquet files, it's a data folder\n",
    "                    if len(parquet_files) >= DISCOVERY_CONFIG[\"min_files\"]:\n",
    "                        folder_info = {\n",
    "                            \"container\": container['name'],\n",
    "                            \"folder_path\": folder_path,\n",
    "                            \"relative_path\": relative_path,\n",
    "                            \"parquet_files\": len(parquet_files),\n",
    "                            \"sample_files\": parquet_files[:3],  # First 3 files as sample\n",
    "                            \"table_name\": generate_table_name(container['name'], relative_path),\n",
    "                            \"depth\": current_depth\n",
    "                        }\n",
    "                        folder_results.append(folder_info)\n",
    "                        print(f\"   üìÑ Data folder: {relative_path or '/'} ({len(parquet_files)} files)\")\n",
    "                    \n",
    "                    # Explore subfolders\n",
    "                    for subfolder_path, subfolder_name in subfolders:\n",
    "                        new_relative_path = f\"{relative_path}/{subfolder_name}\" if relative_path else subfolder_name\n",
    "                        subfolder_results = explore_folder(subfolder_path, current_depth + 1, new_relative_path)\n",
    "                        folder_results.extend(subfolder_results)\n",
    "                    \n",
    "                except Exception as folder_error:\n",
    "                    print(f\"   ‚ö†Ô∏è Could not explore {relative_path}: {str(folder_error)[:50]}...\")\n",
    "                \n",
    "                return folder_results\n",
    "            \n",
    "            # Start exploration from container root\n",
    "            container_folders = explore_folder(container_path)\n",
    "            all_data_folders.extend(container_folders)\n",
    "            \n",
    "            print(f\"   ‚úÖ Found {len(container_folders)} data folders in {container['name']}\")\n",
    "        \n",
    "        discovery_results[\"data_folders\"] = all_data_folders\n",
    "        \n",
    "        print(f\"\\nüìä Total Data Folders Discovered: {len(all_data_folders)}\")\n",
    "        return all_data_folders\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Data folder discovery failed: {str(e)}\"\n",
    "        discovery_results[\"errors\"].append(error_msg)\n",
    "        print(f\"‚ùå {error_msg}\")\n",
    "        return []\n",
    "\n",
    "def generate_table_name(container_name, relative_path):\n",
    "    \"\"\"\n",
    "    Generate a clean table name from container and folder path\n",
    "    \"\"\"\n",
    "    # Clean up the path to create a valid table name\n",
    "    if relative_path:\n",
    "        # Replace special characters and join with underscores\n",
    "        clean_path = relative_path.replace(\"/\", \"_\").replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "        clean_path = ''.join(c for c in clean_path if c.isalnum() or c == '_')\n",
    "        table_name = f\"{DISCOVERY_CONFIG['table_prefix']}{container_name}_{clean_path}\"\n",
    "    else:\n",
    "        table_name = f\"{DISCOVERY_CONFIG['table_prefix']}{container_name}\"\n",
    "    \n",
    "    # Ensure table name is valid (lowercase, no consecutive underscores)\n",
    "    table_name = table_name.lower()\n",
    "    while \"__\" in table_name:\n",
    "        table_name = table_name.replace(\"__\", \"_\")\n",
    "    \n",
    "    return table_name.strip(\"_\")\n",
    "\n",
    "# Execute data folder discovery\n",
    "if containers:\n",
    "    data_folders = discover_data_folders(containers)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No containers found, skipping data folder discovery\")\n",
    "    data_folders = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94be6d1d",
   "metadata": {},
   "source": [
    "## üîó Step 3: Create Shortcuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fc51dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lakehouse shortcuts for discovered data folders\n",
    "def create_shortcuts(data_folders):\n",
    "    \"\"\"\n",
    "    Create lakehouse shortcuts for all discovered data folders with workspace ID for auth compliance\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"üîó Creating lakehouse shortcuts with authentication compliance...\")\n",
    "        from notebookutils import mssparkutils\n",
    "        \n",
    "        # Ensure we have workspace ID for auth compliance\n",
    "        current_workspace_id = WORKSPACE_ID\n",
    "        if not current_workspace_id and DISCOVERY_CONFIG['use_workspace_id']:\n",
    "            try:\n",
    "                current_workspace_id = mssparkutils.env.getWorkspaceId()\n",
    "                print(f\"üîç Retrieved Workspace ID: {current_workspace_id}\")\n",
    "            except Exception as workspace_error:\n",
    "                print(f\"‚ö†Ô∏è Could not retrieve workspace ID: {str(workspace_error)}\")\n",
    "                print(\"üí° Proceeding without workspace ID - shortcuts may fail with auth policies\")\n",
    "        \n",
    "        shortcuts_created = []\n",
    "        \n",
    "        for folder in data_folders:\n",
    "            try:\n",
    "                # Define shortcut paths\n",
    "                source_path = folder[\"folder_path\"]\n",
    "                shortcut_name = folder[\"table_name\"]\n",
    "                target_path = f\"/lakehouse/default/Files/shortcuts/{shortcut_name}/\"\n",
    "                \n",
    "                print(f\"   üîó Creating shortcut: {shortcut_name}\")\n",
    "                print(f\"      Source: {source_path}\")\n",
    "                print(f\"      Target: {target_path}\")\n",
    "                \n",
    "                # Create the shortcut with workspace ID for auth compliance\n",
    "                if current_workspace_id and DISCOVERY_CONFIG['use_workspace_id']:\n",
    "                    print(f\"      üîê Using Workspace ID: {current_workspace_id}\")\n",
    "                    result = mssparkutils.lakehouse.create_shortcut(\n",
    "                        source_path=source_path,\n",
    "                        target_path=target_path,\n",
    "                        shortcut_name=shortcut_name,\n",
    "                        workspace_id=current_workspace_id\n",
    "                    )\n",
    "                else:\n",
    "                    print(f\"      ‚ö†Ô∏è Creating shortcut without workspace ID\")\n",
    "                    result = mssparkutils.lakehouse.create_shortcut(\n",
    "                        source_path=source_path,\n",
    "                        target_path=target_path,\n",
    "                        shortcut_name=shortcut_name\n",
    "                    )\n",
    "                \n",
    "                shortcut_info = {\n",
    "                    \"name\": shortcut_name,\n",
    "                    \"source_path\": source_path,\n",
    "                    \"target_path\": target_path,\n",
    "                    \"workspace_id\": current_workspace_id,\n",
    "                    \"status\": \"success\",\n",
    "                    \"result\": str(result)\n",
    "                }\n",
    "                shortcuts_created.append(shortcut_info)\n",
    "                \n",
    "                print(f\"      ‚úÖ Shortcut created successfully\")\n",
    "                \n",
    "            except Exception as shortcut_error:\n",
    "                error_msg = f\"Failed to create shortcut {folder['table_name']}: {str(shortcut_error)}\"\n",
    "                print(f\"      ‚ùå {error_msg}\")\n",
    "                \n",
    "                # Check if error is related to authentication\n",
    "                if \"auth\" in str(shortcut_error).lower() or \"permission\" in str(shortcut_error).lower():\n",
    "                    print(f\"      üí° This may be an authentication issue - ensure workspace ID is correct\")\n",
    "                    print(f\"      üîê Current Workspace ID: {current_workspace_id or 'None'}\")\n",
    "                \n",
    "                shortcut_info = {\n",
    "                    \"name\": folder[\"table_name\"],\n",
    "                    \"source_path\": folder[\"folder_path\"],\n",
    "                    \"workspace_id\": current_workspace_id,\n",
    "                    \"status\": \"failed\",\n",
    "                    \"error\": error_msg\n",
    "                }\n",
    "                shortcuts_created.append(shortcut_info)\n",
    "                discovery_results[\"errors\"].append(error_msg)\n",
    "        \n",
    "        discovery_results[\"shortcuts_created\"] = shortcuts_created\n",
    "        successful_shortcuts = [s for s in shortcuts_created if s[\"status\"] == \"success\"]\n",
    "        \n",
    "        print(f\"\\nüìä Shortcut Creation Summary:\")\n",
    "        print(f\"   ‚úÖ Successful: {len(successful_shortcuts)}\")\n",
    "        print(f\"   ‚ùå Failed: {len(shortcuts_created) - len(successful_shortcuts)}\")\n",
    "        if current_workspace_id:\n",
    "            print(f\"   üîê Used Workspace ID: {current_workspace_id}\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è No Workspace ID used - may cause auth issues\")\n",
    "        \n",
    "        return shortcuts_created\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Shortcut creation failed: {str(e)}\"\n",
    "        discovery_results[\"errors\"].append(error_msg)\n",
    "        print(f\"‚ùå {error_msg}\")\n",
    "        return []\n",
    "\n",
    "# Execute shortcut creation\n",
    "if data_folders:\n",
    "    shortcuts = create_shortcuts(data_folders)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data folders found, skipping shortcut creation\")\n",
    "    shortcuts = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3754974d",
   "metadata": {},
   "source": [
    "## üìä Step 4: Create External Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548cc9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create external tables for SQL endpoint access\n",
    "def create_external_tables(data_folders):\n",
    "    \"\"\"\n",
    "    Create external Parquet tables for direct SQL endpoint access\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"üìä Creating external Parquet tables...\")\n",
    "        \n",
    "        tables_created = []\n",
    "        \n",
    "        for folder in data_folders:\n",
    "            try:\n",
    "                table_name = folder[\"table_name\"]\n",
    "                source_path = folder[\"folder_path\"]\n",
    "                \n",
    "                print(f\"   üìã Creating table: {table_name}\")\n",
    "                \n",
    "                # Create external Parquet table\n",
    "                create_table_sql = f\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS {table_name}\n",
    "                USING PARQUET\n",
    "                LOCATION '{source_path}'\n",
    "                \"\"\"\n",
    "                \n",
    "                # Execute the SQL command\n",
    "                spark.sql(create_table_sql)\n",
    "                \n",
    "                # Test the table by getting row count\n",
    "                try:\n",
    "                    count_sql = f\"SELECT COUNT(*) as row_count FROM {table_name}\"\n",
    "                    row_count = spark.sql(count_sql).collect()[0]['row_count']\n",
    "                    \n",
    "                    # Get column information\n",
    "                    columns_sql = f\"DESCRIBE {table_name}\"\n",
    "                    columns_df = spark.sql(columns_sql).toPandas()\n",
    "                    column_count = len(columns_df)\n",
    "                    \n",
    "                    table_info = {\n",
    "                        \"name\": table_name,\n",
    "                        \"source_path\": source_path,\n",
    "                        \"status\": \"success\",\n",
    "                        \"row_count\": row_count,\n",
    "                        \"column_count\": column_count,\n",
    "                        \"columns\": columns_df['col_name'].tolist()[:5]  # First 5 columns\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"      ‚úÖ Table created: {row_count:,} rows, {column_count} columns\")\n",
    "                    \n",
    "                except Exception as test_error:\n",
    "                    table_info = {\n",
    "                        \"name\": table_name,\n",
    "                        \"source_path\": source_path,\n",
    "                        \"status\": \"created_but_untested\",\n",
    "                        \"error\": str(test_error)\n",
    "                    }\n",
    "                    print(f\"      ‚ö†Ô∏è Table created but testing failed: {str(test_error)[:50]}...\")\n",
    "                \n",
    "                tables_created.append(table_info)\n",
    "                \n",
    "            except Exception as table_error:\n",
    "                error_msg = f\"Failed to create table {folder['table_name']}: {str(table_error)}\"\n",
    "                print(f\"      ‚ùå {error_msg}\")\n",
    "                \n",
    "                table_info = {\n",
    "                    \"name\": folder[\"table_name\"],\n",
    "                    \"source_path\": folder[\"folder_path\"],\n",
    "                    \"status\": \"failed\",\n",
    "                    \"error\": error_msg\n",
    "                }\n",
    "                tables_created.append(table_info)\n",
    "                discovery_results[\"errors\"].append(error_msg)\n",
    "        \n",
    "        discovery_results[\"tables_created\"] = tables_created\n",
    "        successful_tables = [t for t in tables_created if t[\"status\"] == \"success\"]\n",
    "        \n",
    "        print(f\"\\nüìä Table Creation Summary:\")\n",
    "        print(f\"   ‚úÖ Successful: {len(successful_tables)}\")\n",
    "        print(f\"   ‚ùå Failed: {len([t for t in tables_created if t['status'] == 'failed'])}\")\n",
    "        print(f\"   ‚ö†Ô∏è Created but untested: {len([t for t in tables_created if t['status'] == 'created_but_untested'])}\")\n",
    "        \n",
    "        return tables_created\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Table creation failed: {str(e)}\"\n",
    "        discovery_results[\"errors\"].append(error_msg)\n",
    "        print(f\"‚ùå {error_msg}\")\n",
    "        return []\n",
    "\n",
    "# Execute table creation\n",
    "if data_folders:\n",
    "    tables = create_external_tables(data_folders)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data folders found, skipping table creation\")\n",
    "    tables = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4f98d9",
   "metadata": {},
   "source": [
    "## üìã Step 5: Verification & Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed709c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify created tables and generate summary report\n",
    "def generate_summary_report():\n",
    "    \"\"\"\n",
    "    Generate a comprehensive summary report of the discovery and mapping process\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"üìã Generating Summary Report...\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Overall statistics\n",
    "        total_containers = len(discovery_results[\"containers\"])\n",
    "        total_data_folders = len(discovery_results[\"data_folders\"])\n",
    "        total_shortcuts = len([s for s in discovery_results[\"shortcuts_created\"] if s[\"status\"] == \"success\"])\n",
    "        total_tables = len([t for t in discovery_results[\"tables_created\"] if t[\"status\"] == \"success\"])\n",
    "        total_errors = len(discovery_results[\"errors\"])\n",
    "        \n",
    "        print(f\"üìä {STORAGE_ACCOUNT.upper()} DISCOVERY & MAPPING SUMMARY\")\n",
    "        print(f\"üìÖ Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"üì¶ Storage Account: {STORAGE_ACCOUNT}\")\n",
    "        print(f\"üè¢ Workspace ID: {WORKSPACE_ID or 'Not available'}\")\n",
    "        print(f\"üìÑ Data Format: {DATA_FORMAT}\")\n",
    "        print(f\"üè∑Ô∏è Table Prefix: {DISCOVERY_CONFIG['table_prefix']}\")\n",
    "        print(f\"üîê Auth Compliance: {'Enabled' if DISCOVERY_CONFIG['use_workspace_id'] else 'Disabled'}\")\n",
    "        print()\n",
    "        print(f\"üìà Discovery Results:\")\n",
    "        print(f\"   üì¶ Containers Found: {total_containers}\")\n",
    "        print(f\"   üìÅ Data Folders: {total_data_folders}\")\n",
    "        print(f\"   üîó Shortcuts Created: {total_shortcuts}\")\n",
    "        print(f\"   üìä Tables Created: {total_tables}\")\n",
    "        print(f\"   ‚ùå Errors: {total_errors}\")\n",
    "        \n",
    "        # Shortcut authentication summary\n",
    "        if discovery_results.get(\"shortcuts_created\"):\n",
    "            shortcuts_with_workspace = [s for s in discovery_results[\"shortcuts_created\"] \n",
    "                                       if s.get(\"workspace_id\") and s[\"status\"] == \"success\"]\n",
    "            print(f\"   üîê Shortcuts with Workspace ID: {len(shortcuts_with_workspace)}\")\n",
    "            if WORKSPACE_ID:\n",
    "                print(f\"   üÜî Used Workspace ID: {WORKSPACE_ID}\")\n",
    "        \n",
    "        # Container details\n",
    "        if discovery_results[\"containers\"]:\n",
    "            print(f\"\\nüì¶ Container Details:\")\n",
    "            for i, container in enumerate(discovery_results[\"containers\"], 1):\n",
    "                print(f\"   {i}. {container['name']} ({container.get('size', 'unknown')} items)\")\n",
    "        \n",
    "        # Data folder details\n",
    "        if discovery_results[\"data_folders\"]:\n",
    "            print(f\"\\nüìÅ Data Folder Details:\")\n",
    "            for i, folder in enumerate(discovery_results[\"data_folders\"], 1):\n",
    "                print(f\"   {i}. {folder['container']}{folder['relative_path']} ‚Üí {folder['table_name']}\")\n",
    "                print(f\"      üìÑ {folder['parquet_files']} {DATA_FORMAT} files\")\n",
    "        \n",
    "        # Table verification\n",
    "        print(f\"\\nüß™ Table Verification:\")\n",
    "        try:\n",
    "            all_tables = spark.sql(\"SHOW TABLES\").collect()\n",
    "            prefix_tables = [t for t in all_tables if t.tableName.startswith(DISCOVERY_CONFIG['table_prefix'])]\n",
    "            \n",
    "            print(f\"   üìä Total {DISCOVERY_CONFIG['table_prefix']} tables in SQL endpoint: {len(prefix_tables)}\")\n",
    "            \n",
    "            if prefix_tables:\n",
    "                print(f\"   üìã Available {DISCOVERY_CONFIG['table_prefix']} tables:\")\n",
    "                for table in prefix_tables:\n",
    "                    try:\n",
    "                        # Quick row count test\n",
    "                        count_result = spark.sql(f\"SELECT COUNT(*) as count FROM {table.tableName}\").collect()[0]\n",
    "                        row_count = count_result['count']\n",
    "                        print(f\"      ‚úÖ {table.tableName}: {row_count:,} rows\")\n",
    "                    except Exception as test_error:\n",
    "                        print(f\"      ‚ùå {table.tableName}: Test failed ({str(test_error)[:30]}...)\")\n",
    "        \n",
    "        except Exception as verification_error:\n",
    "            print(f\"   ‚ö†Ô∏è Table verification failed: {str(verification_error)[:50]}...\")\n",
    "        \n",
    "        # Error summary\n",
    "        if discovery_results[\"errors\"]:\n",
    "            print(f\"\\n‚ùå Error Summary:\")\n",
    "            for i, error in enumerate(discovery_results[\"errors\"], 1):\n",
    "                print(f\"   {i}. {error}\")\n",
    "        \n",
    "        # Success summary\n",
    "        if total_tables > 0:\n",
    "            print(f\"\\nüéâ SUCCESS! {total_tables} tables are now available in the SQL endpoint\")\n",
    "            print(f\"üìù You can now query these tables using standard SQL:\")\n",
    "            print(f\"   SELECT * FROM {DISCOVERY_CONFIG['table_prefix']}your_table_name LIMIT 100\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è No tables were successfully created. Check errors above.\")\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Save results to file\n",
    "        save_results_to_file()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Report generation failed: {str(e)}\")\n",
    "\n",
    "def save_results_to_file():\n",
    "    \"\"\"\n",
    "    Save discovery results to a JSON file for future reference\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results_with_timestamp = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"storage_account\": STORAGE_ACCOUNT,\n",
    "            \"data_format\": DATA_FORMAT,\n",
    "            \"config\": DISCOVERY_CONFIG,\n",
    "            \"results\": discovery_results\n",
    "        }\n",
    "        \n",
    "        # Convert to pandas DataFrame for easy viewing\n",
    "        results_df = pd.DataFrame({\n",
    "            \"Container\": [f[\"container\"] for f in discovery_results[\"data_folders\"]],\n",
    "            \"Folder_Path\": [f[\"relative_path\"] for f in discovery_results[\"data_folders\"]],\n",
    "            \"Table_Name\": [f[\"table_name\"] for f in discovery_results[\"data_folders\"]],\n",
    "            f\"{DATA_FORMAT}_Files\": [f[\"parquet_files\"] for f in discovery_results[\"data_folders\"]],\n",
    "            \"Full_Path\": [f[\"folder_path\"] for f in discovery_results[\"data_folders\"]]\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nüìä Discovery Results DataFrame:\")\n",
    "        print(results_df.to_string(index=False))\n",
    "        \n",
    "        # Save as JSON (commented out to avoid file system issues in Fabric)\n",
    "        # filename = f\"{STORAGE_ACCOUNT}_discovery_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        # with open(filename, 'w') as f:\n",
    "        #     json.dump(results_with_timestamp, f, indent=2)\n",
    "        # print(f\"üíæ Results saved to: {filename}\")\n",
    "        \n",
    "        return results_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not save results to file: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Generate the final report\n",
    "results_df = generate_summary_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81642f6",
   "metadata": {},
   "source": [
    "## üöÄ Quick Usage Guide\n",
    "\n",
    "After running this script, you can:\n",
    "\n",
    "### Query Your Data\n",
    "```sql\n",
    "-- List all tables with your configured prefix\n",
    "SHOW TABLES LIKE '{DISCOVERY_CONFIG['table_prefix']}*'\n",
    "\n",
    "-- Query a specific table\n",
    "SELECT * FROM {DISCOVERY_CONFIG['table_prefix']}your_table_name LIMIT 100\n",
    "\n",
    "-- Get table statistics\n",
    "DESCRIBE {DISCOVERY_CONFIG['table_prefix']}your_table_name\n",
    "```\n",
    "\n",
    "### Verify Table Access\n",
    "```python\n",
    "# List all tables with your configured prefix\n",
    "prefix_tables = spark.sql(\"SHOW TABLES\").filter(col(\"tableName\").startswith(\"{DISCOVERY_CONFIG['table_prefix']}\")).collect()\n",
    "for table in prefix_tables:\n",
    "    print(f\"Table: {table.tableName}\")\n",
    "```\n",
    "\n",
    "### Authentication & Workspace Configuration\n",
    "```python\n",
    "# Check current workspace ID\n",
    "from notebookutils import mssparkutils\n",
    "current_workspace_id = mssparkutils.env.getWorkspaceId()\n",
    "print(f\"Current Workspace ID: {current_workspace_id}\")\n",
    "\n",
    "# Manually set workspace ID if needed\n",
    "# WORKSPACE_ID = \"your-workspace-id-here\"\n",
    "```\n",
    "\n",
    "### Troubleshooting\n",
    "- Check the error summary above for any issues\n",
    "- Verify your access permissions to {STORAGE_ACCOUNT}\n",
    "- Ensure the storage account name and containers are correct\n",
    "- **Authentication Issues**: If shortcuts fail, verify workspace ID is correct\n",
    "- **Permission Errors**: Ensure workspace has access to the storage account\n",
    "- Run individual cells to debug specific steps\n",
    "\n",
    "### Configuration Notes\n",
    "- **Storage Account**: {STORAGE_ACCOUNT}\n",
    "- **Workspace ID**: {WORKSPACE_ID or 'Auto-detected'}\n",
    "- **Data Format**: {DATA_FORMAT}\n",
    "- **Table Prefix**: {DISCOVERY_CONFIG['table_prefix']}\n",
    "- **Search Depth**: {DISCOVERY_CONFIG['max_depth']} levels\n",
    "- **File Extensions**: {', '.join(DISCOVERY_CONFIG['file_extensions'])}\n",
    "- **Auth Compliance**: {'Enabled' if DISCOVERY_CONFIG['use_workspace_id'] else 'Disabled'}\n",
    "\n",
    "### Manual Workspace ID Setup\n",
    "If automatic detection fails, you can manually set the workspace ID:\n",
    "```python\n",
    "# Option 1: Set in configuration cell\n",
    "WORKSPACE_ID = \"your-workspace-id-here\"\n",
    "\n",
    "# Option 2: Get from Fabric portal URL\n",
    "# The workspace ID is in the URL: https://app.fabric.microsoft.com/groups/WORKSPACE_ID/...\n",
    "```\n",
    "\n",
    "---\n",
    "**Note**: This notebook automatically discovers and maps all {DATA_FORMAT} data in {STORAGE_ACCOUNT} to your Fabric SQL endpoint using workspace ID for authentication compliance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
