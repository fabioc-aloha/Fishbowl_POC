# Microsoft Fabric Integration Guide for MSIT Environment

## ‚úÖ **CONNECTION ESTABLISHED** - Ready for Data Exploration

This guide documents the successful integration of `cpestaginglake` Azure Data Lake Storage Gen2 with Microsoft Fabric and provides next steps for data exploration and analytics development.

**Integration Status**: ‚úÖ **CONNECTED AND OPERATIONAL**
**Connection Date**: August 7, 2025

**Environment Details:**
- **Fabric URL**: https://msit.powerbi.com.mcas.ms/
- **Workspace Name**: Fishbowl_POC
- **Workspace ID**: 1dfcfdc6-64ff-4338-8eec-2676ff0f5884
- **Storage Account**: cpestaginglake (in integration resource group)
- **Security**: Enterprise-grade with MCAS (Microsoft Cloud App Security) proxy

## Prerequisites

### 1. Access Requirements
- Access to Microsoft Fabric workspace "Fishbowl_POC"
- Permissions to create shortcuts and lakehouses in Fabric
- Azure Storage Blob Data Reader role on cpestaginglake

### 2. Authentication Setup
```powershell
# Ensure you're logged into the correct Azure subscription
az account show --query "{subscription: name, user: user.name, tenant: tenantId}"

# If needed, login to Azure
az login

# Set the correct subscription
az account set --subscription "f6ab5f6d-606a-4256-aba7-1feeeb53784f"
```

## Step 1: Verify Storage Account Access

### Check Storage Account Details
```powershell
# Run the verification script
.\scripts\grant-fabric-storage-permissions.ps1 -Operation check -WorkspaceId "1dfcfdc6-64ff-4338-8eec-2676ff0f5884"
```

### Expected Output
- ‚úÖ Storage account `cpestaginglake` found
- ‚úÖ Current user has appropriate permissions
- ‚úÖ Storage containers are accessible

## Step 2: Grant Fabric Permissions

### Run Permission Script
```powershell
# Grant permissions to your user account for Fabric integration
.\scripts\grant-fabric-storage-permissions.ps1 -Operation grant -WorkspaceId "1dfcfdc6-64ff-4338-8eec-2676ff0f5884"
```

### Manual Permission Grant (if script fails)
If the automated script doesn't work due to MSIT environment restrictions:

1. **Azure Portal Method**:
   - Navigate to [Azure Portal](https://portal.azure.com)
   - Go to Storage Account `cpestaginglake`
   - Click "Access Control (IAM)"
   - Add role assignment: "Storage Blob Data Reader"
   - Assign to your user account

2. **PowerShell Method**:
   ```powershell
   # Get your user ID
   $userId = az ad signed-in-user show --query objectId -o tsv

   # Grant Storage Blob Data Reader role
   az role assignment create `
       --role "Storage Blob Data Reader" `
       --assignee $userId `
       --scope "/subscriptions/f6ab5f6d-606a-4256-aba7-1feeeb53784f/resourceGroups/integration/providers/Microsoft.Storage/storageAccounts/cpestaginglake"
   ```

## ‚úÖ INTEGRATION COMPLETE - Connection Verified

**Status Update (August 7, 2025):**
- ‚úÖ **Storage Connected**: cpestaginglake successfully linked to Fabric workspace
- ‚úÖ **OneLake Shortcuts**: Active and operational
- ‚úÖ **Authentication verified**: `fabioc@microsoft.com`
- ‚úÖ **4 containers accessible**: `aas-container`, `machinelearning`, `synapse`, `test`
- ‚úÖ **Permissions working**: Storage Blob Data Reader role active
- üöÄ **Ready for data exploration and analytics development**

---

## üîç NOW AVAILABLE: Data Exploration & Analytics

### Access Your Connected Workspace
1. **Open Fabric**: Navigate to https://msit.powerbi.com.mcas.ms/
2. **Enter Workspace**: Click on "Fishbowl_POC" workspace (ID: 1dfcfdc6-64ff-4338-8eec-2676ff0f5884)
3. **View Lakehouse**: Access "SynapseDataLake" lakehouse with connected shortcuts

### ‚úÖ Active OneLake Shortcuts - Ready to Explore

Your storage containers are now accessible through OneLake shortcuts:

| Container | Status | Shortcut Path | Ready For |
|-----------|--------|---------------|-----------|
| **üìÅ synapse** | ‚úÖ Connected | `/Files/synapse` | **Data exploration, ETL analysis** |
| **üìÅ machinelearning** | ‚úÖ Connected | `/Files/ml` | **ML model review, artifact analysis** |
| **üìÅ aas-container** | ‚úÖ Connected | `/Files/aas` | **Analysis Services data exploration** |
| **üìÅ test** | ‚úÖ Connected | `/Files/test` | **Sample data testing, validation** |

## üîç Immediate Actions: Start Exploring Your Data

### 1. Explore Data Structure
```
Navigate to your lakehouse:
1. Go to SynapseDataLake lakehouse
2. Click on "Files" section
3. Browse through the connected shortcuts:
   - /synapse/ ‚Üí Main ETL data and processed datasets
   - /ml/ ‚Üí Machine learning models and experiments
   - /aas/ ‚Üí Analysis Services cubes and data
   - /test/ ‚Üí Sample and validation datasets
```

### 2. Data Discovery & Profiling
```
For each container, start exploring:
1. Browse folder structure to understand data organization
2. Preview file contents (CSV, JSON, Parquet files)
3. Check file sizes and modification dates
4. Document interesting datasets for further analysis
```

### 3. Quick Data Analysis
```sql
-- Use SQL endpoint to explore data
SELECT TOP 100 * FROM [SynapseDataLake].[Files].[synapse]
WHERE name LIKE '%.parquet'

-- List all files in synapse container
SELECT path, name, size, modified_date
FROM [SynapseDataLake].[Files]
WHERE path LIKE '%synapse%'
ORDER BY size DESC

-- Explore machine learning artifacts
SELECT * FROM [SynapseDataLake].[Files].[ml]
WHERE name LIKE '%.pkl' OR name LIKE '%.model'
```

### 4. Create Exploration Notebook
```python
# Sample Python notebook for data exploration
import pandas as pd
from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder.appName("DataExploration").getOrCreate()

# Explore synapse data
synapse_files = spark.read.option("recursiveFileLookup", "true") \
    .option("pathGlobFilter", "*.parquet") \
    .parquet("Files/synapse/")

print(f"Synapse data structure:")
synapse_files.printSchema()
synapse_files.show(20)

# Explore machine learning artifacts
ml_path = "Files/ml/"
# List available ML models and experiments
# Document findings for team review
```

## ‚úÖ Integration Success - What You Can Do Now
    displayName = "SynapseDataLake"
    description = "Integration with CPE Synapse staging lake storage"
} | ConvertTo-Json

# Create lakehouse
$response = Invoke-RestMethod -Uri "https://api.fabric.microsoft.com/v1/workspaces/1dfcfdc6-64ff-4338-8eec-2676ff0f5884/lakehouses" -Method POST -Headers $fabricHeaders -Body $lakehouseBody
```

## Step 5: Test Integration - VALIDATION STEPS

### 5.1 Verify Data Access
```
1. Browse Shortcuts: In your lakehouse, navigate through the shortcuts
2. Preview Data: Click on files to preview content
3. Check Folder Structure: Verify you can see Synapse folder hierarchy
4. Test File Access: Open a sample file to confirm read permissions
```

### 5.2 Test SQL Endpoint
```sql
-- Query data from linked Synapse storage
SELECT TOP 100 * FROM [SynapseDataLake].[Files].[synapse].[your_data_folder]

-- List available files
SELECT * FROM [SynapseDataLake].[Files] WHERE path LIKE '%synapse%'

-- Check container contents
SELECT path, name, size FROM [SynapseDataLake].[Files]
WHERE path LIKE '%synapse%'
ORDER BY size DESC
```

### 5.3 Create Sample Notebook
```python
# Sample Python notebook to test integration
import pandas as pd
from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder.appName("SynapseDataTest").getOrCreate()

# Read from OneLake shortcut
df = spark.read.option("multiline", "true").json("Files/synapse/")
df.show(10)

# Convert to Pandas for analysis
pandas_df = df.toPandas()
print(f"Total records: {len(pandas_df)}")
```

## Troubleshooting

### Common Issues

1. **"Workspace not found" Error**:
   - Verify workspace ID: `1dfcfdc6-64ff-4338-8eec-2676ff0f5884`
   - Ensure you have access to the workspace
   - Check if using correct Fabric environment URL

2. **"Insufficient permissions" Error**:
   - Verify Storage Blob Data Reader role assignment
   - Check Azure AD authentication status
   - Ensure storage account key access is not required

3. **MCAS Proxy Issues**:
   - The `mcas.ms` suffix indicates Cloud App Security proxy
   - May require additional authentication steps
   - Contact IT admin if experiencing persistent access issues

### Getting Help

1. **Check Azure Portal**: Verify resource access in Azure Portal
2. **Review Permissions**: Ensure proper RBAC roles are assigned
3. **Contact IT Support**: For MSIT environment-specific issues

## Security Considerations

### MSIT Environment Security
- All connections use Azure AD authentication
- MCAS proxy provides additional security monitoring
- Storage account has key-based access disabled (enterprise security)
- All access is logged and audited

### Best Practices
- Use least-privilege access (Storage Blob Data Reader)
- Regularly review permissions
- Monitor access logs in Azure Monitor
- Follow Microsoft IT security policies

## üöÄ CURRENT PHASE: Data Exploration & Analytics Development

### Phase 1: Data Discovery (NOW - THIS WEEK)
1. **Explore Data Assets**:
   - ‚úÖ Access established: Go to https://msit.powerbi.com.mcas.ms/ ‚Üí Fishbowl_POC workspace
   - üîç **Start here**: Browse `/synapse` container for main ETL datasets
   - üìä **Analyze content**: Document interesting files and data structures
   - üß™ **Test queries**: Use SQL endpoint to explore data samples

2. **Data Profiling & Assessment**:
   - üìà **Profile datasets**: Understand data quality, volume, and structure
   - üóÇÔ∏è **Map data lineage**: Connect files to Synapse pipelines and processes
   - ‚ö° **Test performance**: Measure query response times and data access speed
   - üìã **Document findings**: Create inventory of valuable datasets

3. **Initial Analytics Development**:
   - üìì **Create notebooks**: Develop Python/Spark notebooks for data exploration
   - üìä **Build sample dashboards**: Create initial Power BI reports
   - üßÆ **Test calculations**: Validate data accuracy against known results
   - üë• **Share insights**: Present findings to stakeholders

### Phase 2: Production Analytics (NEXT WEEK)
1. **Advanced Analytics Development**:
   - üîÑ **Automate pipelines**: Create data processing workflows in Fabric
   - üéØ **Develop KPIs**: Build business metrics and performance indicators
   - üö® **Implement monitoring**: Set up data quality checks and alerts
   - üîÑ **Optimize performance**: Tune queries and data access patterns

2. **User Enablement**:
   - üë®‚Äçüè´ **Create training materials**: Document how to use Fabric workspace
   - üéì **Schedule training**: Plan sessions for team members
   - üìñ **Write user guides**: Self-service analytics documentation
   - üéØ **Define use cases**: Specific business scenarios and workflows

### Phase 3: Enterprise Integration (MONTH 2)
1. **Scale & Optimization**:
   - üåê **Expand data sources**: Connect additional systems
   - ‚ö° **Performance tuning**: Optimize for larger datasets
   - üîß **Automate maintenance**: Implement automated data refresh
   - üí∞ **Cost optimization**: Monitor and optimize Fabric capacity usage

---

## üìã Quick Reference Card - ACTIVE ENVIRONMENT

**Status**: ‚úÖ **CONNECTED & OPERATIONAL** üöÄ
**Environment**: MSIT Microsoft Fabric with MCAS
**Workspace**: Fishbowl_POC (1dfcfdc6-64ff-4338-8eec-2676ff0f5884)
**Storage**: cpestaginglake.dfs.core.windows.net ‚úÖ **CONNECTED**
**User**: fabioc@microsoft.com
**Integration Date**: August 7, 2025

**Active Connections**:
- ‚úÖ OneLake Shortcuts: Operational and accessible
- ‚úÖ SQL Endpoint: Ready for queries
- ‚úÖ Spark Notebooks: Available for data processing
- ‚úÖ Power BI: Ready for dashboard development

**Quick Access URLs**:
- üè¢ **Fabric Workspace**: https://msit.powerbi.com.mcas.ms/ ‚Üí Fishbowl_POC
- ‚òÅÔ∏è **Azure Portal**: https://portal.azure.com ‚Üí cpestaginglake storage account
- üìÅ **Storage Explorer**: https://cpestaginglake.dfs.core.windows.net/

**Connected Data Containers** (Ready for exploration):
- üìÅ **synapse** (Main ETL data) ‚Üí `/Files/synapse` - **START HERE**
- üìÅ **machinelearning** (ML models) ‚Üí `/Files/ml`
- üìÅ **aas-container** (Analysis Services) ‚Üí `/Files/aas`
- üìÅ **test** (Test datasets) ‚Üí `/Files/test`

**Current Priority**: üîç **Data Exploration & Content Discovery**

---

**Integration Complete**: August 7, 2025
**Version**: 1.0.0 UNNILNILIUM
**Status**: ‚úÖ **OPERATIONAL - START EXPLORING DATA**
